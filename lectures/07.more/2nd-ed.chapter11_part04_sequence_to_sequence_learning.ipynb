{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1nkx7Qmk_-qz"
   },
   "source": [
    "---\n",
    "\n",
    "## 11.5 Beyond text classification: Sequence-to-sequence learning\n",
    "\n",
    "TensorFlow [tutorial](https://www.tensorflow.org/text/tutorials/nmt_with_attention)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "VY-dJ5T__-q_"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import string\n",
    "import random\n",
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqh6bzrt_-q5"
   },
   "source": [
    "### 11.5.1 A machine translation example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxJaoRpjj2Rn"
   },
   "source": [
    "The attention mechanism originated in research in sequence to sequence with RNNs. The idea was to improve the hidden state of the generating RNN by allowing it to access (pay attention to) various parts of the source sequence at each step of the generation. Later on, the move was to remove the RNN mechanism altogether, and only use attention in both encoder and decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L2M5UEMfj2Rn"
   },
   "source": [
    "<!-- <img style=\"height:700px\" src=\"images/nlp/stanford.seq2seq.png\"> -->\n",
    "<img style=\"height:700px\" src=\"https://github.com/jchwenger/AI/blob/main/lectures/07.more/images/nlp/stanford.seq2seq.png?raw=true\">\n",
    "\n",
    "<small>[Chris Manning, CS224N, Stanford](https://web.stanford.edu/class/cs224n/index.html), [lecture 7](https://web.stanford.edu/class/cs224n/slides/cs224n-2022-lecture07-nmt.pdf)  \n",
    "The original paper: [\"Sequence to Sequence Learning with Neural Networks\"](https://arxiv.org/abs/1409.3215)</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aaRuF5bjj2Ro"
   },
   "source": [
    "<!-- <img src=\"images/nlp/seq2seq-nmt-model-fast.gif\"> -->\n",
    "<img src=\"https://github.com/jchwenger/AI/blob/main/lectures/07.more/images/nlp/seq2seq-nmt-model-fast.gif?raw=true\">\n",
    "\n",
    "<small>[Google seq2seq documentation](https://google.github.io/seq2seq/)</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijxqgNhLj2Ro"
   },
   "source": [
    "### The Transformer: who *needs* RNNs anyway??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijxqgNhLj2Ro"
   },
   "source": [
    "<!-- <img src=\"images/transformer/apply_the_transformer_to_machine_translation.gif\"> -->\n",
    "<img src=\"https://github.com/jchwenger/AI/blob/main/lectures/07.more/images/transformer/apply_the_transformer_to_machine_translation.gif?raw=true\">\n",
    "\n",
    "<small>[\"Neural machine translation with a Transformer and Keras\", TensorFlow](https://www.tensorflow.org/text/tutorials/transformer)  \n",
    "The paper: [Vaswani et al, \"Attention Is All You Need\"](https://arxiv.org/abs/1706.03762)</small>\n",
    "\n",
    "[Andrew Ng, DeepLearning.ai | C5W3L07 Attention Model Intuition](https://www.youtube.com/watch?v=SysgYptB198)  \n",
    "[Andrew Ng, DeepLearning.ai | C5W3L08 Attention Model](https://www.youtube.com/watch?v=quoGRI-1l0A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UtkfIF35j2Ro"
   },
   "source": [
    "### The Transformer applied to translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHblx6uc_-q7"
   },
   "source": [
    "### Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "opgwPWxrkHOj",
    "outputId": "b26b395a-754f-44a4-95f4-182201cf1495"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-09-09 22:15:18--  http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.175.207, 74.125.24.207, 142.251.10.207, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.175.207|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2638744 (2.5M) [application/zip]\n",
      "Saving to: ‘spa-eng.zip’\n",
      "\n",
      "spa-eng.zip         100%[===================>]   2.52M  2.06MB/s    in 1.2s    \n",
      "\n",
      "2024-09-09 22:15:19 (2.06 MB/s) - ‘spa-eng.zip’ saved [2638744/2638744]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATASET_DIR = pathlib.Path(\"spa-eng\")\n",
    "\n",
    "if not DATASET_DIR.exists():\n",
    "    !wget http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
    "    !unzip -q spa-eng.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5uUr_cYj2Ro"
   },
   "source": [
    "Other languages available [here](https://www.manythings.org/anki/)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5wGQedVc_-rC",
    "outputId": "bbcb84fb-830e-4723-81dc-821f5348eb8e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118964/118964 [00:00<00:00, 805850.23it/s]\n"
     ]
    }
   ],
   "source": [
    "TEXT_FILE = DATASET_DIR / \"spa.txt\"\n",
    "\n",
    "with TEXT_FILE.open() as f:\n",
    "    lines = f.read().strip().split(\"\\n\")\n",
    "\n",
    "text_pairs = []\n",
    "separator = \"\\t\" if \"\\t\" in lines[0] else \" \" # spa.txt is tab separated, but the other datasets use spaces\n",
    "\n",
    "for line in tqdm(lines):\n",
    "    english, spanish = line.split(separator)[:2] # for datasets other than the official one, only tak\n",
    "    spanish = \"[start] \" + spanish + \" [end]\"\n",
    "    text_pairs.append((english, spanish))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P1punR8b_-rF",
    "outputId": "0f248622-4eed-49c0-fa70-1d5e38843f44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Let's leave as soon as he gets back.\", '[start] Vayámonos tan pronto como él vuelva. [end]')\n"
     ]
    }
   ],
   "source": [
    "print(random.choice(text_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "O-1pdxOW_-rH"
   },
   "outputs": [],
   "source": [
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHnC14bP_-rK"
   },
   "source": [
    "#### Vectorizing the English and Spanish text pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "-uVEJ1Na_-rL"
   },
   "outputs": [],
   "source": [
    "strip_chars = string.punctuation + \"¿\"\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(\n",
    "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
    "\n",
    "vocab_size = 15000\n",
    "sequence_length = 20\n",
    "\n",
    "source_vectorization = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "target_vectorization = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length + 1,\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "train_english_texts = [pair[0] for pair in train_pairs]\n",
    "train_spanish_texts = [pair[1] for pair in train_pairs]\n",
    "source_vectorization.adapt(train_english_texts)\n",
    "target_vectorization.adapt(train_spanish_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OK4GQ9m1_-rQ"
   },
   "source": [
    "#### Preparing datasets for the translation task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "_Bzz_hbi_-rS"
   },
   "outputs": [],
   "source": [
    "batch_size = 64 # my 6GB GPU can go up to 1560\n",
    "\n",
    "def format_dataset(eng, spa):\n",
    "    eng = source_vectorization(eng)\n",
    "    spa = target_vectorization(spa)\n",
    "    return ({\n",
    "        \"english\": eng,\n",
    "        \"spanish\": spa[:, :-1],\n",
    "    }, spa[:, 1:])\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    eng_texts, spa_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    spa_texts = list(spa_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LL1oTABJ_-rV",
    "outputId": "693661d0-92ec-4d6f-fa0f-4a9c185c021c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs['english'].shape: (64, 20), dtype: <dtype: 'int64'>\n",
      "inputs['spanish'].shape: (64, 20), dtype: <dtype: 'int64'>\n",
      "targets.shape: (64, 20), dtype: <dtype: 'int64'>\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds:\n",
    "    print(f\"inputs['english'].shape: {inputs['english'].shape}, dtype: {inputs['english'].dtype}\")\n",
    "    print(f\"inputs['spanish'].shape: {inputs['spanish'].shape}, dtype: {inputs['spanish'].dtype}\")\n",
    "    print(f\"targets.shape: {targets.shape}, dtype: {targets.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_fLq0Uo_-rX"
   },
   "source": [
    "---\n",
    "\n",
    "### 11.5.2 Sequence-to-sequence learning with RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qb87ZNyj_-rX"
   },
   "source": [
    "#### GRU-based encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "JnDDTPgM_-rZ"
   },
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "latent_dim = 1024\n",
    "\n",
    "source = tf.keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
    "x = tf.keras.layers.Embedding(vocab_size, embed_dim, mask_zero=True)(source)\n",
    "encoded_source = tf.keras.layers.Bidirectional(\n",
    "    tf.keras.layers.GRU(latent_dim), merge_mode=\"sum\"\n",
    ")(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yPRe7pO_-rb"
   },
   "source": [
    "#### GRU-based decoder and the end-to-end model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "qJ3-1wM1_-rc"
   },
   "outputs": [],
   "source": [
    "past_target = tf.keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
    "x = tf.keras.layers.Embedding(vocab_size, embed_dim, mask_zero=True)(past_target)\n",
    "decoder_gru = tf.keras.layers.GRU(latent_dim, return_sequences=True)\n",
    "x = decoder_gru(x, initial_state=encoded_source)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "target_next_step = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "seq2seq_rnn = tf.keras.Model([source, past_target], target_next_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVsP-y06_-re"
   },
   "source": [
    "#### Training our recurrent sequence-to-sequence model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NKY4zb5h_-rf",
    "outputId": "bed479cb-cfce-4dbf-c5a3-ac7a43e07216"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 33ms/step - accuracy: 0.1549 - loss: 5.2578 - val_accuracy: 0.1563 - val_loss: 3.8969\n",
      "Epoch 2/15\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 33ms/step - accuracy: 0.1604 - loss: 3.8769 - val_accuracy: 0.1873 - val_loss: 3.2604\n",
      "Epoch 3/15\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 33ms/step - accuracy: 0.1864 - loss: 3.3111 - val_accuracy: 0.2073 - val_loss: 2.8829\n",
      "Epoch 4/15\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 33ms/step - accuracy: 0.2031 - loss: 2.9355 - val_accuracy: 0.2204 - val_loss: 2.6351\n",
      "Epoch 5/15\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 33ms/step - accuracy: 0.2177 - loss: 2.6413 - val_accuracy: 0.2316 - val_loss: 2.4531\n",
      "Epoch 6/15\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 33ms/step - accuracy: 0.2297 - loss: 2.4078 - val_accuracy: 0.2395 - val_loss: 2.3223\n",
      "Epoch 7/15\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 33ms/step - accuracy: 0.2402 - loss: 2.2137 - val_accuracy: 0.2457 - val_loss: 2.2220\n",
      "Epoch 8/15\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 33ms/step - accuracy: 0.2488 - loss: 2.0490 - val_accuracy: 0.2500 - val_loss: 2.1583\n",
      "Epoch 9/15\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 33ms/step - accuracy: 0.2565 - loss: 1.9084 - val_accuracy: 0.2539 - val_loss: 2.1007\n",
      "Epoch 10/15\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 33ms/step - accuracy: 0.2632 - loss: 1.7881 - val_accuracy: 0.2571 - val_loss: 2.0504\n",
      "Epoch 11/15\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 33ms/step - accuracy: 0.2694 - loss: 1.6772 - val_accuracy: 0.2593 - val_loss: 2.0200\n",
      "Epoch 12/15\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 33ms/step - accuracy: 0.2746 - loss: 1.5842 - val_accuracy: 0.2610 - val_loss: 1.9963\n",
      "Epoch 13/15\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 33ms/step - accuracy: 0.2798 - loss: 1.4994 - val_accuracy: 0.2626 - val_loss: 1.9717\n",
      "Epoch 14/15\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 33ms/step - accuracy: 0.2840 - loss: 1.4263 - val_accuracy: 0.2636 - val_loss: 1.9540\n",
      "Epoch 15/15\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 33ms/step - accuracy: 0.2880 - loss: 1.3570 - val_accuracy: 0.2653 - val_loss: 1.9385\n"
     ]
    }
   ],
   "source": [
    "seq2seq_rnn.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "history = seq2seq_rnn.fit(train_ds, epochs=15,validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLo0IzRO_-rh"
   },
   "source": [
    "#### Translating new sentences with our RNN encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "5LmRctMwlw_K"
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = target_vectorization([decoded_sentence])\n",
    "        next_token_predictions = seq2seq_rnn.predict(\n",
    "            [tokenized_input_sentence, tokenized_target_sentence], verbose=0)\n",
    "        sampled_token_index = np.argmax(next_token_predictions[0, i, :])\n",
    "        sampled_token = spa_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "MYEsny7m_-ri"
   },
   "outputs": [],
   "source": [
    "spa_vocab = target_vectorization.get_vocabulary()\n",
    "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
    "max_decoded_sentence_length = 20\n",
    "test_eng_texts = [pair[0] for pair in test_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gy_3EXyC_-rj",
    "outputId": "16640f0e-6cde-4cb4-9365-f36b3d7ca889"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Tom never smiles.\n",
      "[start] tom nunca se ve [end]\n",
      "-\n",
      "Excuse me, I didn't catch your name.\n",
      "[start] perdone no me tu nombre [end]\n",
      "-\n",
      "I barely even remember Tom.\n",
      "[start] apenas me he dado tom [end]\n",
      "-\n",
      "We have had a bad time.\n",
      "[start] hemos tenido mal tiempo [end]\n",
      "-\n",
      "You're incompetent.\n",
      "[start] eres [UNK] [end]\n",
      "-\n",
      "Tom had a good teacher.\n",
      "[start] tom tuvo un buen profesor [end]\n",
      "-\n",
      "Tom found out that his phone had been tapped.\n",
      "[start] tom encontró que había perdido su billetera [end]\n",
      "-\n",
      "The wound left a scar on his arm.\n",
      "[start] la herida le puso una cicatriz en el brazo [end]\n",
      "-\n",
      "Tom fed the neighbor's cat while they were away.\n",
      "[start] tom se quitó los platos y el gato se había ido [end]\n",
      "-\n",
      "I think we'll be safe here.\n",
      "[start] creo que estamos aquí para estar acá [end]\n",
      "-\n",
      "My roommate is crazy.\n",
      "[start] mi compañero de habitación es [end]\n",
      "-\n",
      "They asked me to make a speech on short notice.\n",
      "[start] me pidió que [UNK] un discurso más alto [end]\n",
      "-\n",
      "Don't change your mind.\n",
      "[start] no [UNK] tu promesa [end]\n",
      "-\n",
      "The moon has come out.\n",
      "[start] la noche se ha ido [end]\n",
      "-\n",
      "You look as healthy as ever.\n",
      "[start] te ves tan rápido como tú [end]\n",
      "-\n",
      "Can you do it alone?\n",
      "[start] puedes hacerlo solo [end]\n",
      "-\n",
      "She was a rather prim and proper young lady.\n",
      "[start] ella fue un poco de [UNK] y [UNK] y [UNK] [end]\n",
      "-\n",
      "Can you remember the first time you ate at this restaurant?\n",
      "[start] te puedes acordar de la primera vez en este restaurante [end]\n",
      "-\n",
      "The peacock's beautiful tail helps it attract females.\n",
      "[start] la [UNK] [UNK] el [UNK] [UNK] [UNK] de la [UNK] [end]\n",
      "-\n",
      "Don't call me anymore.\n",
      "[start] no me llame más [end]\n"
     ]
    }
   ],
   "source": [
    "for _ in range(20):\n",
    "    input_sentence = random.choice(test_eng_texts)\n",
    "    print(\"-\")\n",
    "    print(input_sentence)\n",
    "    print(decode_sequence(input_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UgaX2pz__-rl"
   },
   "source": [
    "---\n",
    "\n",
    "### 11.5.3 Sequence-to-sequence learning with Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ldxfx1_G_-rm"
   },
   "source": [
    "#### Putting it all together: A Transformer for machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9zh7clK_-rn"
   },
   "source": [
    "<!-- <img style=\"\" src=\"images/transformer/the-annotated-transformer_14_0.transformer-full.png\"> -->\n",
    "<img src=\"https://github.com/jchwenger/AI/blob/main/lectures/07.more/images/transformer/the-annotated-transformer_14_0.transformer-full.png?raw=true\">\n",
    "\n",
    "<small>[Vaswani et al, \"Attention Is All You Need\"](https://arxiv.org/abs/1706.03762)</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4o2f_Wp_-rn"
   },
   "source": [
    "#### The Transformer encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "8QuSCuu1CW3p"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim                               # parameters\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "    \n",
    "    # new in Keras 3, see: https://keras.io/guides/making_new_layers_and_models_via_subclassing/#best-practice-deferring-weight-creation-until-the-shape-of-the-inputs-is-known\n",
    "    def build(self, input_shape):\n",
    "        self.attention = tf.keras.layers.MultiHeadAttention(     # multi-head attention layer\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = tf.keras.Sequential(                   # dense layer on top: like a nonlinearity\n",
    "            [tf.keras.layers.Dense(dense_dim, activation=\"relu\"),\n",
    "             tf.keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = tf.keras.layers.LayerNormalization()  # layer norm\n",
    "        self.layernorm_2 = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:                                     # optional mask\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "        attention_output = self.attention(\n",
    "            inputs, inputs, attention_mask=mask                  # only two inputs! Value is used as key as well\n",
    "        )\n",
    "        proj_input = self.layernorm_1(inputs + attention_output) # inputs + attn: residual connection\n",
    "        proj_output = self.dense_proj(proj_input)                # dense layer on top: like a nonlinearity\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JmzaAotVj2Rr"
   },
   "source": [
    "#### The Transformer decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mntQSHWK_-ro",
    "outputId": "16b186de-89b8-4dde-9c05-1d0b2248e235"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      "tf.Tensor(\n",
      "[[41  0 24 34 22 28 41 25  3 16]\n",
      " [44 21 42 13  8 36 32 28 37 11]], shape=(2, 10), dtype=int32)\n",
      "\n",
      "i:\n",
      "[[0]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [4]\n",
      " [5]\n",
      " [6]\n",
      " [7]\n",
      " [8]\n",
      " [9]]\n",
      "\n",
      "j:\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "\n",
      "Is i >= j? Boolean cast to ints. (Note the broadcasting)\n",
      "\n",
      "tf.Tensor(\n",
      "[[1 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 0 0 0 0 0 0 0]\n",
      " [1 1 1 1 0 0 0 0 0 0]\n",
      " [1 1 1 1 1 0 0 0 0 0]\n",
      " [1 1 1 1 1 1 0 0 0 0]\n",
      " [1 1 1 1 1 1 1 0 0 0]\n",
      " [1 1 1 1 1 1 1 1 0 0]\n",
      " [1 1 1 1 1 1 1 1 1 0]\n",
      " [1 1 1 1 1 1 1 1 1 1]], shape=(10, 10), dtype=int32)\n",
      "\n",
      "We want mask to have the same dims as input, using `tf.tile`.\n",
      "Creating the right multiplier for it:\n",
      "\n",
      "tf.Tensor([2 1 1], shape=(3,), dtype=int32)\n",
      "\n",
      "Final mask with batch dimensions:\n",
      "\n",
      "tf.Tensor(\n",
      "[[[1 0 0 0 0 0 0 0 0 0]\n",
      "  [1 1 0 0 0 0 0 0 0 0]\n",
      "  [1 1 1 0 0 0 0 0 0 0]\n",
      "  [1 1 1 1 0 0 0 0 0 0]\n",
      "  [1 1 1 1 1 0 0 0 0 0]\n",
      "  [1 1 1 1 1 1 0 0 0 0]\n",
      "  [1 1 1 1 1 1 1 0 0 0]\n",
      "  [1 1 1 1 1 1 1 1 0 0]\n",
      "  [1 1 1 1 1 1 1 1 1 0]\n",
      "  [1 1 1 1 1 1 1 1 1 1]]\n",
      "\n",
      " [[1 0 0 0 0 0 0 0 0 0]\n",
      "  [1 1 0 0 0 0 0 0 0 0]\n",
      "  [1 1 1 0 0 0 0 0 0 0]\n",
      "  [1 1 1 1 0 0 0 0 0 0]\n",
      "  [1 1 1 1 1 0 0 0 0 0]\n",
      "  [1 1 1 1 1 1 0 0 0 0]\n",
      "  [1 1 1 1 1 1 1 0 0 0]\n",
      "  [1 1 1 1 1 1 1 1 0 0]\n",
      "  [1 1 1 1 1 1 1 1 1 0]\n",
      "  [1 1 1 1 1 1 1 1 1 1]]], shape=(2, 10, 10), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "def get_causal_attention_mask(inputs):\n",
    "    print(\"Inputs:\")\n",
    "    print(inputs)\n",
    "    print()\n",
    "    input_shape = tf.shape(inputs)\n",
    "    batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "    i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "    j = tf.range(sequence_length)\n",
    "    print(f\"i:\\n{i}\")\n",
    "    print()\n",
    "    print(f\"j:\\n{j}\")\n",
    "    print()\n",
    "    mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "    print(\"Is i >= j? Boolean cast to ints. (Note the broadcasting)\")\n",
    "    print()\n",
    "    print(mask)\n",
    "    print()\n",
    "    mask = tf.reshape(mask, (1, input_shape[1], input_shape[1])) # adding a batch dimension\n",
    "    mult = tf.concat(\n",
    "        [tf.expand_dims(batch_size, -1),\n",
    "         tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
    "    print(\"We want mask to have the same dims as input, using `tf.tile`.\")\n",
    "    print(\"Creating the right multiplier for it:\")\n",
    "    print()\n",
    "    print(mult)\n",
    "    print()\n",
    "    tile = tf.tile(mask, mult)\n",
    "    print(\"Final mask with batch dimensions:\")\n",
    "    print()\n",
    "    print(tile)\n",
    "    return tile\n",
    "\n",
    "mask = get_causal_attention_mask(tf.random.uniform(shape=(2,10), maxval=50, dtype=tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "o0EgikJE_-rp"
   },
   "outputs": [],
   "source": [
    "class TransformerDecoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim                              # parameters\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.supports_masking = True                            # MASK: enforcing causality\n",
    "    \n",
    "    # new in Keras 3, see: https://keras.io/guides/making_new_layers_and_models_via_subclassing/#best-practice-deferring-weight-creation-until-the-shape-of-the-inputs-is-known\n",
    "    def build(self, input_shape):\n",
    "        self.attention_1 = tf.keras.layers.MultiHeadAttention(  # multi-head attention\n",
    "            num_heads=self.num_heads, key_dim=self.embed_dim\n",
    "        )\n",
    "        self.attention_2 = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=self.num_heads, key_dim=self.embed_dim)\n",
    "        self.dense_proj = tf.keras.Sequential(                  # dense layer on top: like a nonlinearity\n",
    "            [tf.keras.layers.Dense(self.dense_dim, activation=\"relu\"),\n",
    "             tf.keras.layers.Dense(self.embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = tf.keras.layers.LayerNormalization() # layer norm\n",
    "        self.layernorm_2 = tf.keras.layers.LayerNormalization()\n",
    "        self.layernorm_3 = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    # retrieve config as a dict (necessary for custom Keras layers)\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1),\n",
    "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
    "        return tf.tile(mask, mult)\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)               # prepare the causal mask\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(\n",
    "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "        else:\n",
    "            padding_mask = mask\n",
    "\n",
    "        attention_output_1 = self.attention_1(                             # REGULAR MASKED ATTENTION\n",
    "            query=inputs,\n",
    "            value=inputs,\n",
    "            key=inputs,\n",
    "            attention_mask=causal_mask)                                    # first layer: apply the causal mask\n",
    "        attention_output_1 = self.layernorm_1(inputs + attention_output_1) # layer norm\n",
    "\n",
    "        attention_output_2 = self.attention_2(                             # CROSS-ATTENTION\n",
    "            query=attention_output_1,                                      # query: output of DECODER\n",
    "            value=encoder_outputs,                                         # key:   output of ENCODER\n",
    "            key=encoder_outputs,                                           # value: output of ENCODER\n",
    "            attention_mask=padding_mask,\n",
    "        )\n",
    "        attention_output_2 = self.layernorm_2(                             # layer norm: concatenate both attention matrices\n",
    "            attention_output_1 + attention_output_2)                       # cross-attention (encoder) + regular masked attention (decoder)\n",
    "\n",
    "        proj_output = self.dense_proj(attention_output_2)                  # dense net / nonlinearity\n",
    "        return self.layernorm_3(attention_output_2 + proj_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xdiOCciN_-rq"
   },
   "source": [
    "#### PositionalEmbedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "aMpWoOO6_-rq"
   },
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.sequence_length = sequence_length                   # more params\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    # new in Keras 3, see: https://keras.io/guides/making_new_layers_and_models_via_subclassing/#best-practice-deferring-weight-creation-until-the-shape-of-the-inputs-is-known\n",
    "    def build(self, input_shape):\n",
    "        self.token_embeddings = tf.keras.layers.Embedding(       # token embeddings: semantic information\n",
    "            input_dim=self.input_dim, output_dim=self.output_dim\n",
    "        )\n",
    "        self.position_embeddings = tf.keras.layers.Embedding(    # position embeddings: syntactic/spatial information\n",
    "            input_dim=self.sequence_length, output_dim=self.output_dim\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        embedded_tokens = self.token_embeddings(inputs)          # 1. create token embeddings\n",
    "                                                                 # 2. create pos embeddings\n",
    "        positions = tf.range(start=0, limit=length, delta=1)     #    (as many as our input length, delta: step size)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "\n",
    "        return embedded_tokens + embedded_positions              # 3. Both embeddings are simply added together!\n",
    "\n",
    "    # copied from the source here: https://github.com/keras-team/keras-nlp/blob/4601d88a61a5d3d15279865769af5155804dd785/keras_nlp/src/layers/modeling/token_and_position_embedding.py#L146\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return self.token_embeddings.compute_mask(inputs, mask=mask)\n",
    "\n",
    "    def get_config(self):                                        # retrieve config as a dict\n",
    "        config = super().get_config()                            # (required for Keras layers)\n",
    "        config.update({\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nx0_Kqft_-rs"
   },
   "source": [
    "#### End-to-end Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bV3vlIOQ_-rt",
    "outputId": "f35aef11-b463-4c83-c2f8-0786fe1e0209"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:915: UserWarning: Layer 'transformer_encoder' (of type TransformerEncoder) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 256\n",
    "dense_dim = 2048\n",
    "num_heads = 8\n",
    "\n",
    "encoder_inputs = tf.keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "\n",
    "decoder_inputs = tf.keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "decoder_outputs = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "transformer = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CA7IVIh_-ru"
   },
   "source": [
    "#### Training the sequence-to-sequence Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "id": "hN0LibyckgTl",
    "outputId": "7308d625-a226-49bb-8453-4fc3ef123be9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ english (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ spanish (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ positional_embedding      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,845,120</span> │ english[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbedding</span>)     │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ not_equal_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ english[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ positional_embedding_1    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,845,120</span> │ spanish[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbedding</span>)     │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ transformer_encoder       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,155,456</span> │ positional_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)      │                        │                │ not_equal_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ transformer_decoder       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">5,259,520</span> │ positional_embedding_… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecoder</span>)      │                        │                │ transformer_encoder[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ transformer_decoder[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15000</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,855,000</span> │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ english (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ spanish (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ positional_embedding      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │      \u001b[38;5;34m3,845,120\u001b[0m │ english[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "│ (\u001b[38;5;33mPositionalEmbedding\u001b[0m)     │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ not_equal_2 (\u001b[38;5;33mNotEqual\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ english[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ positional_embedding_1    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │      \u001b[38;5;34m3,845,120\u001b[0m │ spanish[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "│ (\u001b[38;5;33mPositionalEmbedding\u001b[0m)     │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ transformer_encoder       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │      \u001b[38;5;34m3,155,456\u001b[0m │ positional_embedding[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)      │                        │                │ not_equal_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ transformer_decoder       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │      \u001b[38;5;34m5,259,520\u001b[0m │ positional_embedding_… │\n",
       "│ (\u001b[38;5;33mTransformerDecoder\u001b[0m)      │                        │                │ transformer_encoder[\u001b[38;5;34m0\u001b[0m… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ transformer_decoder[\u001b[38;5;34m0\u001b[0m… │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15000\u001b[0m)    │      \u001b[38;5;34m3,855,000\u001b[0m │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,960,216</span> (76.14 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m19,960,216\u001b[0m (76.14 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,960,216</span> (76.14 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m19,960,216\u001b[0m (76.14 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transformer.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Jyvm4QJc0hy",
    "outputId": "02926e2e-4421-4df4-b553-57c623e228b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:915: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:915: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:915: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 37ms/step - accuracy: 0.1416 - loss: 4.7831 - val_accuracy: 0.2249 - val_loss: 2.8105\n",
      "Epoch 2/30\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 10ms/step - accuracy: 0.2327 - loss: 2.7821 - val_accuracy: 0.2583 - val_loss: 2.2176\n",
      "Epoch 3/30\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 11ms/step - accuracy: 0.2597 - loss: 2.2817 - val_accuracy: 0.2679 - val_loss: 2.0555\n",
      "Epoch 4/30\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 11ms/step - accuracy: 0.2726 - loss: 2.0610 - val_accuracy: 0.2733 - val_loss: 1.9942\n",
      "Epoch 5/30\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 11ms/step - accuracy: 0.2806 - loss: 1.9310 - val_accuracy: 0.2739 - val_loss: 1.9865\n",
      "Epoch 6/30\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 11ms/step - accuracy: 0.2858 - loss: 1.8552 - val_accuracy: 0.2756 - val_loss: 1.9748\n",
      "Epoch 7/30\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 11ms/step - accuracy: 0.2898 - loss: 1.8026 - val_accuracy: 0.2771 - val_loss: 1.9978\n",
      "Epoch 8/30\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 11ms/step - accuracy: 0.2933 - loss: 1.7601 - val_accuracy: 0.2793 - val_loss: 1.9974\n",
      "Epoch 9/30\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - accuracy: 0.2958 - loss: 1.7288 - val_accuracy: 0.2781 - val_loss: 2.0301\n",
      "Epoch 10/30\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - accuracy: 0.2982 - loss: 1.6989 - val_accuracy: 0.2778 - val_loss: 2.0548\n",
      "Epoch 11/30\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - accuracy: 0.3003 - loss: 1.6736 - val_accuracy: 0.2800 - val_loss: 2.0421\n",
      "Epoch 12/30\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - accuracy: 0.3026 - loss: 1.6471 - val_accuracy: 0.2798 - val_loss: 2.0770\n",
      "Epoch 13/30\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - accuracy: 0.3044 - loss: 1.6257 - val_accuracy: 0.2768 - val_loss: 2.1234\n",
      "Epoch 14/30\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - accuracy: 0.3060 - loss: 1.6032 - val_accuracy: 0.2799 - val_loss: 2.1162\n",
      "Epoch 15/30\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - accuracy: 0.3074 - loss: 1.5825 - val_accuracy: 0.2779 - val_loss: 2.1651\n",
      "Epoch 16/30\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - accuracy: 0.3090 - loss: 1.5623 - val_accuracy: 0.2789 - val_loss: 2.1757\n",
      "Epoch 17/30\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - accuracy: 0.3104 - loss: 1.5446 - val_accuracy: 0.2796 - val_loss: 2.1742\n",
      "Epoch 18/30\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - accuracy: 0.3121 - loss: 1.5247 - val_accuracy: 0.2796 - val_loss: 2.2096\n",
      "Epoch 19/30\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - accuracy: 0.3133 - loss: 1.5057 - val_accuracy: 0.2784 - val_loss: 2.2303\n",
      "Epoch 20/30\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - accuracy: 0.3143 - loss: 1.4919 - val_accuracy: 0.2802 - val_loss: 2.2215\n",
      "Epoch 21/30\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - accuracy: 0.3155 - loss: 1.4767 - val_accuracy: 0.2798 - val_loss: 2.2631\n",
      "Epoch 22/30\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - accuracy: 0.3169 - loss: 1.4624 - val_accuracy: 0.2781 - val_loss: 2.3096\n",
      "Epoch 23/30\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - accuracy: 0.3176 - loss: 1.4499 - val_accuracy: 0.2788 - val_loss: 2.2950\n",
      "Epoch 24/30\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - accuracy: 0.3185 - loss: 1.4343 - val_accuracy: 0.2789 - val_loss: 2.3097\n",
      "Epoch 25/30\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - accuracy: 0.3196 - loss: 1.4213 - val_accuracy: 0.2781 - val_loss: 2.3272\n",
      "Epoch 26/30\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - accuracy: 0.3206 - loss: 1.4098 - val_accuracy: 0.2792 - val_loss: 2.3431\n",
      "Epoch 27/30\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - accuracy: 0.3217 - loss: 1.3995 - val_accuracy: 0.2790 - val_loss: 2.3742\n",
      "Epoch 28/30\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - accuracy: 0.3222 - loss: 1.3887 - val_accuracy: 0.2787 - val_loss: 2.3769\n",
      "Epoch 29/30\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - accuracy: 0.3228 - loss: 1.3800 - val_accuracy: 0.2789 - val_loss: 2.4117\n",
      "Epoch 30/30\n",
      "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - accuracy: 0.3233 - loss: 1.3739 - val_accuracy: 0.2777 - val_loss: 2.4388\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7dae804475b0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.fit(train_ds, epochs=30, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NBZyrX47_-rw"
   },
   "source": [
    "#### Translating new sentences with our Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "oLMBgLNSmR2q"
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = target_vectorization(\n",
    "            [decoded_sentence])[:, :-1]\n",
    "        predictions = transformer(\n",
    "            [tokenized_input_sentence, tokenized_target_sentence])\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = spa_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "_h34FTG__-rx"
   },
   "outputs": [],
   "source": [
    "spa_vocab = target_vectorization.get_vocabulary()\n",
    "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
    "max_decoded_sentence_length = 20\n",
    "test_eng_texts = [pair[0] for pair in test_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BlC2opUzl3dv",
    "outputId": "26495717-86e5-4ebf-c302-dc54d6343cb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "He retired at the age of 65.\n",
      "[start] Él se quitó a la edad de el asesinato [end]\n",
      "-\n",
      "He wanted to be a farmer.\n",
      "[start] Él quería ser un granjero [end]\n",
      "-\n",
      "I never drink wine.\n",
      "[start] nunca bebo [end]\n",
      "-\n",
      "The person we're trying to catch is very dangerous.\n",
      "[start] la persona estaba tratando de alcanzar a la peligroso [end]\n",
      "-\n",
      "Since my mother was sick, I couldn't go there.\n",
      "[start] como mi madre estaba enfermo porque no pude ir ahí [end]\n",
      "-\n",
      "The girl resembled her mother.\n",
      "[start] la niña se miró a su madre [end]\n",
      "-\n",
      "\"I forgot,\" she answered.\n",
      "[start] me olvidé respondió [end]\n",
      "-\n",
      "Tighten this screw.\n",
      "[start] [UNK] esto [end]\n",
      "-\n",
      "Drive safely.\n",
      "[start] [UNK] a salvo [end]\n",
      "-\n",
      "We had fish for supper last night.\n",
      "[start] nos pasamos el pasado por la noche [end]\n",
      "-\n",
      "The evidence speaks for itself.\n",
      "[start] la evidencia habla por si mismo [end]\n",
      "-\n",
      "Beat the egg whites until stiff.\n",
      "[start] el mucha guerra con un montón de tiempo [end]\n",
      "-\n",
      "I just need a little time.\n",
      "[start] solo necesito un momento [end]\n",
      "-\n",
      "They caught foxes with traps.\n",
      "[start] ellos dijeron perder verdad [end]\n",
      "-\n",
      "That's what Tom was looking for.\n",
      "[start] esto es lo que tom estaba buscando [end]\n",
      "-\n",
      "She went out to buy some food.\n",
      "[start] a ella le fue a comprar comida [end]\n",
      "-\n",
      "You're very intelligent.\n",
      "[start] eres muy inteligente [end]\n",
      "-\n",
      "I am a very sad person.\n",
      "[start] estoy muy triste [end]\n",
      "-\n",
      "I'm looking for the same thing you are.\n",
      "[start] estoy buscando lo mismo que eres tú [end]\n",
      "-\n",
      "The boy carved his name in the tree.\n",
      "[start] el muchacho hizo el nombre en el árbol [end]\n"
     ]
    }
   ],
   "source": [
    "for _ in range(20):\n",
    "    input_sentence = random.choice(test_eng_texts)\n",
    "    print(\"-\")\n",
    "    print(input_sentence)\n",
    "    print(decode_sequence(input_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LvUhhvtT_-ry"
   },
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z0GXDH4fj2Rs"
   },
   "source": [
    "## References"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Transformers with Lucas Beyer, Google Brain](https://www.youtube.com/watch?v=EixI6t5oif0)  \n",
    "[Stanford Deep Learning for NLP CS224N (2024 before)](https://www.youtube.com/playlist?list=PLoROMvodv4rOaMFbaqxPDoLWjDaRAdP9D)  \n",
    "[Stanford CS224N: NLP w/ DL | Spring 2024 | Lecture 7 - Attention, Final Projects and LLM Intro](https://www.youtube.com/watch?v=J7ruSOIzhrE&list=PLoROMvodv4rOaMFbaqxPDoLWjDaRAdP9D&index=8) ([2021](https://www.youtube.com/watch?v=wzfWHP6SXxY))  \n",
    "[Stanford CS224N NLP with Deep Learning | 2023 | Lecture 8 - Self-Attention and Transformers](https://www.youtube.com/watch?v=LWMzyfvuehA&list=PLoROMvodv4rOaMFbaqxPDoLWjDaRAdP9D&index=8)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BefvOBHj2Rt"
   },
   "source": [
    "### Transformers from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BefvOBHj2Rt"
   },
   "source": [
    "[Blog post](https://peterbloem.nl/blog/transformers).\n",
    "\n",
    "[Lecture 12.1 Self-attention](https://www.youtube.com/watch?v=KmAISyVvE1Y&list=PLIXJ-Sacf8u60G1TwcznBmK6rEL3gmZmV) ([more lectures](https://www.youtube.com/@dlvu6202/playlists))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-3T9jJfgj2Rt"
   },
   "source": [
    "### More"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-3T9jJfgj2Rt"
   },
   "source": [
    "[Stanford CS25 Transformers United](https://www.youtube.com/playlist?list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM) covering various research topics & transformers."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Slideshow",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
