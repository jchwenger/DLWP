{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "234a7c55-50f1-4b6f-933c-d85ec4441de3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## 10.3 Understanding recurrent neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088d152d-a4ab-45ea-a942-1bf7f4569c04",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777bdfc5-d2a8-4e16-a67b-392fc7e1312f",
   "metadata": {},
   "source": [
    "'Vanilla' recurrent nets\n",
    "\n",
    "<!-- ![RNN](images/rnn/dprogrammer.RNN.png) -->\n",
    " \n",
    "![RNN](https://github.com/jchwenger/AI/blob/main/lectures/07.more/images/rnn/dprogrammer.RNN.png?raw=true)\n",
    "\n",
    "<small>[\"RNN, LSTM & GRU\", dprogrammer.org](http://dprogrammer.org/rnn-lstm-gru)</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366941c1-b859-4e77-8c0c-682d454612e3",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "$$\n",
    "\\bbox[5px,border:2px solid red]\n",
    "{\n",
    "\\begin{align*}\n",
    "h_t &= \\sigma_h(U_h \\cdot x_t+V_h \\cdot h_{t-1}+b_h)  \\\\\n",
    "o_t &= \\sigma_o(W_o \\cdot h_t+b_o) \n",
    "\\end{align*}\n",
    "}\n",
    "$$\n",
    "\n",
    "$x_t$ : input vector.  \n",
    "$h_t$ : hidden layer vector.  \n",
    "$o_t$ : output vector.  \n",
    "$b_h, b_o$ : bias vectors.  \n",
    "$U,W,V$ : parameter matrices.  \n",
    "$\\sigma_h$: activation, typically $tanh$.  \n",
    "$\\sigma_o$ : activation, $softmax$ or $sigmoid$, depending on your needs.  \n",
    "\n",
    "<small>[\"RNN, LSTM & GRU\", dprogrammer.org](http://dprogrammer.org/rnn-lstm-gru)</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a9164c-8046-40de-97fb-c9d6ac7b6ba3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa2e696-7354-4135-84db-d42d4e818ed7",
   "metadata": {},
   "source": [
    "Fully-fledged recurrent nets\n",
    "\n",
    "<!-- ![LSTM](images/rnn/dprogrammer.LSTM.png) -->\n",
    " \n",
    "![LSTM](https://github.com/jchwenger/AI/blob/main/lectures/07.more/images/rnn/dprogrammer.LSTM.png?raw=true)\n",
    "\n",
    "<small>[\"RNN, LSTM & GRU\", dprogrammer.org](http://dprogrammer.org/rnn-lstm-gru)</small>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e011481-b8aa-4c8f-8b3a-2751229b4ea4",
   "metadata": {},
   "source": [
    "$$\n",
    "\\bbox[5px,border:2px solid red]\n",
    "{\n",
    "\\begin{align*}\n",
    "f_t &= \\sigma(W_f \\cdot h_{t-1} + U_f \\cdot x_t+b_f) \\\\\n",
    "i_t &= \\sigma(W_i \\cdot h_{t-1} + U_i \\cdot x_t+b_i) \\\\\n",
    "o_t &= \\sigma(W_o \\cdot h_{t-1} + U_o \\cdot x_t+b_o) \\\\\n",
    "\\tilde{C}_t &= \\tanh(W_c\\cdot h_{t-1} + U_c \\cdot x_t+b_c) \\\\\n",
    "C_t &= f_t \\odot C_{t-1}+i_t\\odot\\tilde{C}_t \\\\\n",
    "h_t &= o_t \\odot \\tanh(C_t) \n",
    "\\end{align*}\n",
    "}\n",
    "$$\n",
    "\n",
    "$h_t$ , $C_t$ : hidden layer vectors.  \n",
    "$x_t$ : input vector.  \n",
    "$b_f$ , $b_i$ , $b_c$ , $b_o$ : bias vector.  \n",
    "$W_f$ , $W_i$ , $W_c$ , $W_o$ : parameter matrices.  \n",
    "$U_f$ , $U_i$ , $U_c$ , $U_o$ : parameter matrices.  \n",
    "$\\sigma$ , $\\tanh$ : activation functions.  \n",
    "$\\odot$ : the Hadamard (element-wise) product.\n",
    "\n",
    "**Note**\n",
    "\n",
    "$f$ is for *forget*  \n",
    "$c$ is for *carry*  \n",
    "$i$ is for *input*  \n",
    "$o$ is for *output*\n",
    "\n",
    "<small>[\"RNN, LSTM & GRU\", dprogrammer.org](http://dprogrammer.org/rnn-lstm-gru)</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2782080-dac1-412f-a698-5b7ae75ec2dd",
   "metadata": {},
   "source": [
    "<!-- <img src=\"images/rnn/mit.lstm.1.png\"> -->\n",
    "<img src=\"https://github.com/jchwenger/AI/blob/main/lectures/07.more/images/rnn/mit.lstm.1.png?raw=true\">\n",
    "\n",
    "<small>[Ava Soleimany, MIT 6.S191 (2021): Recurrent Neural Networks](https://www.youtube.com/watch?v=qjrad0V0uJE)</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99b65d3-8434-4687-a995-f1adf1faf8a2",
   "metadata": {},
   "source": [
    "<!-- <img src=\"images/rnn/mit.lstm.2.png\"> -->\n",
    "<img src=\"https://github.com/jchwenger/AI/blob/main/lectures/07.more/images/rnn/mit.lstm.2.png?raw=true\">\n",
    "\n",
    "<small>[Ava Soleimany, MIT 6.S191 (2021): Recurrent Neural Networks](https://www.youtube.com/watch?v=qjrad0V0uJE)</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2250c26b-93be-4944-a7e6-69bb4d982081",
   "metadata": {},
   "source": [
    "<!-- <img src=\"images/rnn/mit.lstm.3.png\"> -->\n",
    "<img src=\"https://github.com/jchwenger/AI/blob/main/lectures/07.more/images/rnn/mit.lstm.3.png?raw=true\">\n",
    "\n",
    "<small>[Ava Soleimany, MIT 6.S191 (2021): Recurrent Neural Networks](https://www.youtube.com/watch?v=qjrad0V0uJE)</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e993b87-8cd4-43d9-9d21-1abe96b46b49",
   "metadata": {},
   "source": [
    "<img src=\"images/rnn/mit.lstm.4.png\">\n",
    "<!-- <img src=\"https://github.com/jchwenger/AI/blob/main/lectures/07.more/images/rnn/mit.lstm.4.png?raw=true\"> -->\n",
    "\n",
    "<small>[Ava Soleimany, MIT 6.S191 (2021): Recurrent Neural Networks](https://www.youtube.com/watch?v=qjrad0V0uJE)</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ecff8f-89d0-4f4c-a075-021e897023d4",
   "metadata": {},
   "source": [
    "<!-- <img src=\"images/rnn/mit.lstm.5.png\"> -->\n",
    "<img src=\"https://github.com/jchwenger/AI/blob/main/lectures/07.more/images/rnn/mit.lstm.5.png?raw=true\">\n",
    "\n",
    "<small>[Ava Soleimany, MIT 6.S191 (2021): Recurrent Neural Networks](https://www.youtube.com/watch?v=qjrad0V0uJE)</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa42aea-284c-4894-9a3c-52e9b5b3ac00",
   "metadata": {},
   "source": [
    "<!-- <img src=\"images/rnn/mit.lstm.6.png\"> -->\n",
    "<img src=\"https://github.com/jchwenger/AI/blob/main/lectures/07.more/images/rnn/mit.lstm.6.png?raw=true\">\n",
    "\n",
    "<small>[Ava Soleimany, MIT 6.S191 (2021): Recurrent Neural Networks](https://www.youtube.com/watch?v=qjrad0V0uJE)</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dde0c58-e428-4fb5-93e9-8a4554c0c1b5",
   "metadata": {},
   "source": [
    "<img src=\"images/rnn/mit.lstm.7.png\">\n",
    "<!-- <img src=\"https://github.com/jchwenger/AI/blob/main/lectures/07.more/images/rnn/mit.lstm.7.png?raw=true\"> -->\n",
    "\n",
    "<small>[Ava Soleimany, MIT 6.S191 (2021): Recurrent Neural Networks](https://www.youtube.com/watch?v=qjrad0V0uJE)</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498ac461-c0b6-4eb5-98be-14cb99d50590",
   "metadata": {},
   "source": [
    "<!-- <img src=\"images/rnn/mit.lstm.8.png\"> -->\n",
    "<img src=\"https://github.com/jchwenger/AI/blob/main/lectures/07.more/images/rnn/mit.lstm.8.png?raw=true\">\n",
    "\n",
    "<small>[Ava Soleimany, MIT 6.S191 (2021): Recurrent Neural Networks](https://www.youtube.com/watch?v=qjrad0V0uJE)</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fbdc62-5b22-4613-8763-bcbd108bcb40",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### LSTM pseudocode "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051bdbbe-2100-4e9a-b462-d740bb4fed69",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "memory = gated_prev_memory + gated_simple_RNN\n",
    "output_t = gated_tanh(memory)\n",
    "```\n",
    "\n",
    "- `output_t`, `prev_output`: current and previous LSTM layer outputs\n",
    "- `memory`: aka *carry* or *state* – the conveyor belt, allowing free flow of information from the past (if gates are open) \n",
    "- `simple_RNN`: `tanh(dot(input_t, W) + dot(prev_output, U) + b)`\n",
    "- `simple_RNN`: `sigmoid(dot(input_t, W) + dot(prev_output, U) + b)`\n",
    "- `gated_tanh(memory)`: the memory is squashed to $[-1, 1]$ and gated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8443118-8576-499d-ac6c-66c0fdbbb486",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### The LSTM layer in detail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ea1425-cad1-48e9-b414-d7c3a0597203",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "##### Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8141256-2c44-4ab5-b648-de29f0c6ce8a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Also called *carry* by Chollet and, or *state* by other authors. \n",
    "\n",
    "1. A forget gate:  \n",
    "  $ f_{t} = \\sigma_g(W_f \\cdot x_{t} + U_f \\cdot h_{t - 1} + b_f)$  \n",
    "  $\\sigma_g$ is a sigmoid, outputing in $[0, 1]$, a kind of smoothed gate\n",
    "   \n",
    "2. An input gate:  \n",
    "  $i_{t} = \\sigma_g(W_i \\cdot x_{t} + U_i \\cdot h_{t - 1} + b_i)$  \n",
    "  which we can also imagine as open or closed or in-between.\n",
    "\n",
    "3. A simple RNN layer – to inject new information into the memory  \n",
    "  $k_{t} = \\sigma_h(W_k \\cdot x_{t} + U_k \\cdot h_{t - 1} + b_k)$  \n",
    "  where $\\sigma_h$ is the $\\tanh$ activation.\n",
    "\n",
    "4. Putting all this together:  \n",
    "  `memory = gated_prev_memory + gated_simple_RNN`    \n",
    "  $c_{t} = f_{t} \\odot c_{t - 1} + i_{t} *\\odot k_{t}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251b8f9b-3361-405e-a317-ce9349f6039d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "##### Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfca3ad-7be9-4904-adae-1ff42ade0ce7",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "1. An output gate  \n",
    "  $o_{t} = \\sigma_d(W_o \\cdot x_{t} + U_o \\cdot h_{t - 1} + b_o)$  \n",
    "\n",
    "2. which multiplies, or gates, the memory  \n",
    "  `output_t = output_gate_{t} * tanh(memory)`  \n",
    "  $h_{t} = o_{t} \\odot \\sigma_h(c_{t})$    \n",
    "  $\\sigma_h$ is a hyperbolic tangent (or, in a 'peephole' LSTM, the identity function $\\sigma(x) = x$). \n",
    "  \n",
    "$\\odot$ : the Hadamard (element-wise) product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ffa60b-9042-435c-9304-e984b2a1df69",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0aea24-256f-483c-9e2a-1bd05af8873a",
   "metadata": {},
   "source": [
    "'Optimised' recurrent nets\n",
    "\n",
    "<!-- ![GRU](images/rnn/dprogrammer.GRU.png) -->\n",
    " \n",
    "![LSTM](https://github.com/jchwenger/AI/blob/main/lectures/07.more/images/rnn/dprogrammer.GRU.png?raw=true)\n",
    "\n",
    "<small>[\"RNN, LSTM & GRU\", dprogrammer.org](http://dprogrammer.org/rnn-lstm-gru)</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77eeb2e0-482c-4347-8c6f-a6d5c56fabef",
   "metadata": {},
   "source": [
    "$$\n",
    "\\bbox[5px,border:2px solid red]\n",
    "{\n",
    "\\begin{align*}\n",
    "z_t &= \\sigma(W_z \\cdot h_{t-1} + U_z \\cdot x_t+b_z) \\\\\n",
    "r_t &= \\sigma(W_r \\cdot h_{t-1} + U_r \\cdot x_t+b_r) \\\\\n",
    "\\tilde{h}_t &= \\tanh(W_h \\cdot (r_t \\odot h_{t-1}) + U_h \\cdot x_t+b_h) \\\\\n",
    "h_t &= (1-z_t)\\odot h_{t-1}+z_t \\odot \\tilde{h}_t \n",
    "\\end{align*}\n",
    "}\n",
    "$$\n",
    "\n",
    "\n",
    "$h_t $: hidden layer vectors.  \n",
    "$x_t $: input vector.  \n",
    "$b_z , b_r , b_h $: bias vector.  \n",
    "$W_z , W_r , W_h $: parameter matrices.  \n",
    "$U_z , U_r , U_h $: parameter matrices.  \n",
    "$\\sigma$ , $\\tanh$ : activation functions.\n",
    "$\\odot$ : the Hadamard (element-wise) product.\n",
    "\n",
    "<small>[\"RNN, LSTM & GRU\", dprogrammer.org](http://dprogrammer.org/rnn-lstm-gru)</small>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678367a5-517c-411d-bd8b-ef0783e7b405",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Lecture references"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f58fd5d-7589-4a19-9a4a-25aaa699ca05",
   "metadata": {},
   "source": [
    "[MIT 6.S191 (2021): Recurrent Neural Networks](https://www.youtube.com/watch?v=qjrad0V0uJE)  \n",
    "[Stanford CS224N NLP with Deep Learning | Winter 2021 | Lecture 6 - Simple and LSTM RNNs ](https://www.youtube.com/watch?v=0LixFSa7yts)  \n",
    "[Stanford CS231N | Convolutional Neural Networks for Visual Recognition (Spring 2017)| Lecture 10 | Recurrent Neural Networks](https://www.youtube.com/watch?v=6niqTuYFZLQ)  \n",
    "[Stanford CS224N | Natural Language Processing with Deep Learning (Winter 2017) | Lecture 8: Recurrent Neural Networks and Language Models](https://www.youtube.com/watch?v=Keqep_PKrY8)  \n",
    "[Stanford CS224N | Natural Language Processing with Deep Learning (Winter 2017) | Lecture 9: Machine Translation and Advanced Recurrent LSTMs and GRUs](https://www.youtube.com/watch?v=QuELiw8tbx8)  "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
