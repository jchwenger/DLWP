{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50769ece-ad18-485c-a9ec-467a52b5d395",
   "metadata": {
    "editable": true,
    "id": "385d77cc-5965-4806-a6f7-830b9b64658e",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92084b97-4bfc-45d1-956d-dcefedce045c",
   "metadata": {
    "editable": true,
    "id": "385d77cc-5965-4806-a6f7-830b9b64658e",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# The engine of neural networks: Gradient-based optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44bb1253",
   "metadata": {
    "id": "44bb1253"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0dd58b-b8ee-4157-a920-94add627251a",
   "metadata": {
    "editable": true,
    "id": "9a0dd58b-b8ee-4157-a920-94add627251a",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Each neural layer performs `activation(dot(w, x) + b)`.\n",
    "\n",
    "But what are the optimal values of `w` and `b` for a particular problem?\n",
    "\n",
    "We have seen an example of **training**, allowing the network to find those.\n",
    "\n",
    "<!-- <img src=\"images/chollet/figure1.9.png\" style=\"height: 400px\"> -->\n",
    "<img style=\"height: 400px;\" src=\"https://raw.githubusercontent.com/jchwenger/DLWP/main/lectures/02/images/chollet/figure1.9.png\">\n",
    "\n",
    "<small>[DLWP](https://deeplearningwithpython.io/chapters/chapter01_what-is-deep-learning/), Chapter 1, Figure 1.9</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e4c6c7-d851-4df6-8e1f-5710a11f3a73",
   "metadata": {
    "id": "b4e9d62e"
   },
   "source": [
    "#### Note"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4464e2-0c64-42b8-b94b-9a18cf60e615",
   "metadata": {
    "id": "b4e9d62e",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "The training loop either terminates (reaching the specified number of epochs) or is interrupted (manually).\n",
    "\n",
    "ðŸ’€ There is **no universal formula** for when to stop. (This is an active area of research.)\n",
    "\n",
    "Practitioners tend to save various versions along the way, keeping the best-performing one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e3a4f6-576c-4fc4-a14b-2156f8399e43",
   "metadata": {
    "id": "e3cb83dd"
   },
   "source": [
    "### Gradient descent: our compass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777e2907-23d5-4abe-aad1-173d968216b3",
   "metadata": {
    "id": "e3cb83dd"
   },
   "source": [
    "  - We want to minimise our loss function;\n",
    "  - The output of the loss function can be viewed as the **ground height** in a **multidimensional landscape (the input space)** â€“ this is computed for **each** sample;\n",
    "  - The value of all the weights and biases define a **coordinate** in this landscape;\n",
    "  - The **gradient** gives us the **direction of steepest ascent**: it tells us how to tweak parameters so as to increase our loss the most;\n",
    "  - Going the opposite way is like **taking a step downward** on the **landscape** of the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a56e33-ba02-446a-acca-b634c04b7453",
   "metadata": {
    "id": "9ee035c9"
   },
   "source": [
    "### Backpropagation: our gradient engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893c8a1d-48f3-40c5-a2a4-ec7ea0e3bf8d",
   "metadata": {
    "id": "9ee035c9"
   },
   "source": [
    "But then, we want to know which parameters change our loss the most, to lower our loss as much as possible.\n",
    "\n",
    "By calculating the derivative of **each operation** with respect to the loss, we know how much each of them **affects** the loss.\n",
    "\n",
    "And so by how much we need to change each parameter in order to lower the overall loss the most effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15aad0be-fb6c-48d0-834d-b50b99352d7a",
   "metadata": {
    "id": "9ee035c9",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Note"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f2a956-382f-4724-9c2e-d5affb55ca99",
   "metadata": {
    "id": "9ee035c9"
   },
   "source": [
    "**Everything must be smooth and differentiable**, so we can precisely compute the influence of every operation on the final loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b30caf1-5d71-4f64-90f7-153aba3e2f39",
   "metadata": {
    "id": "3256629e"
   },
   "source": [
    "## What do we need now?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7cc113-df3e-46ad-b58b-f7ac7ee46d1a",
   "metadata": {
    "id": "3256629e"
   },
   "source": [
    "1. **Derivatives**\n",
    "2. Derivatives for tensors: the gradient\n",
    "3. Optimization: stochastic gradient descent, momentum\n",
    "4. Chaining derivatives: the backpropagation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144292f1-5754-4d4e-b0d3-70455816787b",
   "metadata": {
    "id": "bdfe245b-b47f-4a41-89a2-8e3494f3c7e1"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07554ec6-0237-4b32-bc70-fbe017f2cd29",
   "metadata": {
    "id": "bdfe245b-b47f-4a41-89a2-8e3494f3c7e1"
   },
   "source": [
    "## What's a derivative?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8795a411-e69c-4e65-ac77-5a00f8e7a51a",
   "metadata": {
    "id": "8795a411-e69c-4e65-ac77-5a00f8e7a51a"
   },
   "source": [
    "A smooth function $f$ is a function without abrupt changes.\n",
    "\n",
    "$f$ in the vicinity of a point $x$ can always be approximated:\n",
    "\n",
    "\\begin{align*}\n",
    "f(x + \\delta x) \\approx f(x) + m \\delta x\n",
    "\\end{align*}\n",
    "\n",
    "where $m$ is the gradient of the tangent at $x$ and $\\delta x$ is a small step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e433ef2e-328a-4a7d-afe9-a4865d3fa263",
   "metadata": {
    "id": "e433ef2e-328a-4a7d-afe9-a4865d3fa263"
   },
   "source": [
    "<!-- ![derivative.png](images/derivative.png) -->\n",
    "<img style=\"\" src=\"https://raw.githubusercontent.com/jchwenger/DLWP/main/lectures/02/images/derivative.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b95ec7-bf8b-45c8-9381-ed2ddd6cdea7",
   "metadata": {
    "id": "f4b95ec7-bf8b-45c8-9381-ed2ddd6cdea7"
   },
   "source": [
    "The gradient of differentiable functions â€“ those without abrupt changes â€“ is calculable from simple rules.\n",
    "\n",
    "Here are differentiation rules:\n",
    "\n",
    "\n",
    "|$f$ |$\\frac{df}{dx}$ | |\n",
    "|:---|:---|:---|\n",
    "|$x^n$|$nx^{n-1}$| Power |\n",
    "|$g(x) + h(x)$ | $\\frac{dg}{dx} + \\frac{dh}{dx}$| Linearity |\n",
    "|$af(x)$ | $a\\frac{df}{dx}$ | Linearity |\n",
    "|$f(y(x))$ | $\\frac{df}{dy}\\frac{dy}{dx}$ | Chain rule |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d4e707",
   "metadata": {
    "id": "02d4e707"
   },
   "source": [
    "Here are some differentiation examples:\n",
    "\n",
    "|$f$ |$\\frac{df}{dx}$ | |\n",
    "|:---|:---|---|\n",
    "| $5x^3$ | $15 x^2$ | |\n",
    "| $5x^2 - 2x + 3$ | $10x - 2$ | |\n",
    "| $(3x + 2x^2)^2$ | $2(3x + 2x^2)(3 + 4x)$ | chain rule!  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fe2342",
   "metadata": {
    "id": "48fe2342"
   },
   "source": [
    "$$\\color{red}{(}\\color{green}{3x + 2x^2}\\color{red}{)^2}$$\n",
    "\n",
    "can be written as a composition:\n",
    "\n",
    "\\begin{align*}\n",
    "f &= \\color{red}{y^2} \\color{black}{\\text{ where}}\\\\\n",
    "y &= \\color{green}{3x + 2x^2}  \\\\\n",
    "\\end{align*}\n",
    "\n",
    "$f$ is differentiated with respect to $y$, and then $y$ is differentiated with respect to $x$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{df}{dx} &= \\color{red}{\\frac{df}{dy}}\\color{green}{\\frac{dy}{dx}} \\\\\n",
    "               &= \\color{red}{2y}   \\color{green}{(3 + 4x)} \\\\\n",
    "               &= \\color{red}{2(3x + 2x^2)}\\color{green}{(3 + 4x)}\n",
    "\\end{align*}\n",
    "\n",
    "##### Note\n",
    "\n",
    "Complex derivatives (compositions of functions) can be broken down into simpler ones!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afedf08",
   "metadata": {
    "id": "8afedf08"
   },
   "source": [
    "TensorFlow is meant to automate all that.\n",
    "\n",
    "TensorFlow can track operations and calculate gradients with the `GradientTape` object.\n",
    "\n",
    "Then, the method `tape.gradient(df, dx)` takes a function and a point and returns the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b371f9a7",
   "metadata": {
    "id": "b371f9a7"
   },
   "source": [
    "Let's start with a super simple example, the gradient function of $f(x) = x^2$ is $f'(x) = 2x$.\n",
    "\n",
    "The gradient at $x = 3$ is $6$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e8806c0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5e8806c0",
    "outputId": "80ed97e3-08e2-45f0-9626-93f4765bb808"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape:  # GradientTape means: \"please please record operations and please calculate\n",
    "    f = x**2                     # all derivatives automatically for me!\"\n",
    "                                 # https://www.tensorflow.org/api_docs/python/tf/GradientTape\n",
    "df_dx = tape.gradient(f, x) # x can also be an array of variables\n",
    "print(df_dx.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0de4926-f9ff-45c9-94d5-357418483571",
   "metadata": {
    "id": "2f36153f"
   },
   "source": [
    "## Next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc28e2aa-cc8e-4a15-a83b-82791827be98",
   "metadata": {
    "id": "2f36153f"
   },
   "source": [
    "1. Derivatives\n",
    "2. **Derivatives for tensors: the gradient**\n",
    "3. Optimization: stochastic gradient descent, momentum\n",
    "4. Chaining derivatives: the backpropagation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cdb18f-cdd4-44db-859d-4b301ca429be",
   "metadata": {
    "id": "566860ab-714d-40a1-8476-c3f2bb2b129c"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6902724e-2754-459e-b398-ad3e0b7b1d44",
   "metadata": {
    "id": "566860ab-714d-40a1-8476-c3f2bb2b129c"
   },
   "source": [
    "## Derivative of a tensor operation: the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56c13be-c0de-4818-a6c1-e59d109aa601",
   "metadata": {
    "id": "566860ab-714d-40a1-8476-c3f2bb2b129c"
   },
   "source": [
    "But the loss function is tensor-valued...\n",
    "\n",
    "What is the derivative of a tensor operation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c3cfc8-23ee-43e0-8b44-ba1dabc8e8c8",
   "metadata": {
    "id": "64c3cfc8-23ee-43e0-8b44-ba1dabc8e8c8"
   },
   "source": [
    "Here's our affine transformation from last time.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "y = \\left(\\begin{array}{c} y_1 \\\\\n",
    "                       y_2 \\end{array} \\right)\n",
    "=\n",
    "\\left( \\begin{array}{c} w_{11} & w_{12} \\\\\n",
    "                        w_{21} & w_{22} \\end{array} \\right)\n",
    "\\left( \\begin{array}{c} x_1  \\\\\n",
    "                        x_2  \\end{array}  \\right)\n",
    "+\n",
    "\\left( \\begin{array}{c} b_1  \\\\\n",
    "                        b_2         \\end{array}  \\right)\n",
    "\\end{eqnarray}\n",
    "\n",
    "For example:\n",
    "\\begin{eqnarray}\n",
    "\\left( \\begin{array}{c} 1 & -2 \\\\\n",
    "                                 2 & 1  \\end{array}  \\right)\n",
    "\\left( \\begin{array}{c} 1  \\\\\n",
    "                        2  \\end{array}  \\right)\n",
    "+\n",
    "\\left( \\begin{array}{c} 0  \\\\\n",
    "                        -1\\end{array}  \\right)\n",
    "=\n",
    "\\left( \\begin{array}{c}-3  \\\\\n",
    "                        3         \\end{array}  \\right)\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ff76ec6-7c09-470a-9cc9-5a6856897e6d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ff76ec6-7c09-470a-9cc9-5a6856897e6d",
    "outputId": "55007dd5-2b91-42ed-d26a-8c667c26b2d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.]\n",
      " [ 3.]]\n"
     ]
    }
   ],
   "source": [
    "w = tf.Variable([\n",
    "    [1., -2.],\n",
    "    [2.,  1.]\n",
    "])\n",
    "b = tf.Variable([\n",
    "    [ 0.],\n",
    "    [-1.]\n",
    "])\n",
    "x = tf.Variable([\n",
    "    [1.],\n",
    "    [2.]\n",
    "])\n",
    "\n",
    "y = w @ x + b # @ is the matrix multiplication operator\n",
    "\n",
    "print(y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acbd180-bbd9-46e1-b90c-f80633e579be",
   "metadata": {
    "id": "5acbd180-bbd9-46e1-b90c-f80633e579be"
   },
   "source": [
    "The gradient of a two variable function, $f = f(x, y)$, with respect to either variable, is written $\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}...$ (with 'curly' d's).\n",
    "\n",
    "We almost have all we need to explain automatic gradient descent â€“ how the optimiser tweaks layer weights and biases in an efficient implementation known as backpropagation.\n",
    "\n",
    "The final bit of theory is **partial differentiation**.\n",
    "\n",
    "Partial differentiation means the differentiation of a function of several variables.\n",
    "\n",
    "\n",
    "Here, $f$ is a function of two variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e582977-9464-44aa-a699-0af36e673d45",
   "metadata": {
    "id": "5e582977-9464-44aa-a699-0af36e673d45"
   },
   "source": [
    "Differentiation follows the single-variable rules.\n",
    "\n",
    "The **undifferentiated variable is held constant**:\n",
    "\n",
    "| $f$|$\\frac{\\partial f}{\\partial x}$ |$\\frac{\\partial f}{\\partial y}$ |\n",
    "|:---|:---|:---|\n",
    "|$xy$|$y$| $x$ |\n",
    "| $ax^n + by^m$|$anx^{n-1}$| $bmy^{m-1}$ |\n",
    "\n",
    "These are literally the same the rules for one variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6026c62-a3c8-4103-9634-11244dae00f7",
   "metadata": {
    "id": "d6026c62-a3c8-4103-9634-11244dae00f7"
   },
   "source": [
    "Suppose we have a very simple network that implements a single 2D affine transformation.\n",
    "\n",
    "\\begin{align*}\\\\\n",
    "Input&:\\ (x_1, x_2)\\\\\n",
    "Prediction&:\\ (y_1, y_2)\\\\\n",
    "Target&:\\ (0, 0)\\\\\n",
    "\\bf{Loss}&:\\ (y_1 - 0)^2 + (y_2 - 0)^2 = \\mathbf{y_1^2 + y_2^2}\\ \\leftarrow squared\\ distance\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c56e18a-6974-41d3-ab9a-a7df6cecb738",
   "metadata": {
    "id": "5c56e18a-6974-41d3-ab9a-a7df6cecb738"
   },
   "source": [
    "We wish to adjust the weights in order to lower the loss on the input $(1, 2)^T$.\n",
    "\n",
    "We need to find the gradients in $w, b$-space.\n",
    "\n",
    "We must differentiate with respect to $w_{11}, w_{12}, w_{21}, w_{22}, b_1, b_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645ecabc-784b-4c16-ab66-cddba161d0e9",
   "metadata": {
    "id": "645ecabc-784b-4c16-ab66-cddba161d0e9"
   },
   "source": [
    "\\begin{align*}\n",
    "y_1 &= w_{11}x_1 + w_{12}x_2 + b_1 \\\\\n",
    "y_2 &= w_{21}x_1 + w_{22}x_2  + b_2 \\\\\n",
    "\\\\\n",
    "J^* &= y_1^2 + y_2^2 &\\\\\n",
    "  &= (w_{11}x_1 + w_{12}x_2 + b_1)^2 + (w_{21}x_1 + w_{22}x_2  + b_2)^2\n",
    "\\end{align*}\n",
    "\n",
    "\\* **_J_** is commonly used for the loss (think:ob*J*ective? It actually comes from the [Jacobian matrix](https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant), the matrix of partial derivatives of a vector-valued function)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6e2b0f-fc20-48d4-b960-aa36066d12bc",
   "metadata": {
    "id": "1e6e2b0f-fc20-48d4-b960-aa36066d12bc"
   },
   "source": [
    "The loss is a fairly horrible function of the weights and biases. Here for $w_{11}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9195c2df",
   "metadata": {
    "id": "9195c2df"
   },
   "source": [
    "\\begin{align*}\n",
    "\\frac{\\partial J}{\\partial w_{11}} &= \\frac{\\partial J}{\\partial y_1} \\frac{\\partial y_1}{\\partial w_{11}}\\ &&|\\ J = y_1^2 + y_2^2 \\\\\n",
    "                                   &= \\frac{\\partial (\\color{red}{y_1^2} + \\color{blue}{y_2^2})}{\\partial y_1}  \\frac{\\partial y_1}{\\partial w_{11}}\\ &&|\\ only\\ y_1^2\\ remains \\\\\n",
    "                                   &= 2\\color{red}{y_1} \\frac{\\partial y_1}{\\partial w_{11}}\\ &&|\\ y_1 = w_{11}x_1 + w_{12}x_2 + b_1 \\\\\n",
    "                                   &= 2y_1 \\frac{\\partial (\\color{red}{w_{11}x_1} + \\color{blue}{w_{12}x_2} + \\color{blue}{b_1})}{\\partial w_{11}}\\ &&|\\ only\\ w_{11}x_1\\ remains,\\ x_1\\ treated\\ as\\ a\\ constant  \\\\\n",
    "                                   &= \\underline{2y_1 \\color{red}{x_1}}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818a7268-693a-4772-94d7-fe193fe95c8a",
   "metadata": {
    "id": "818a7268-693a-4772-94d7-fe193fe95c8a"
   },
   "source": [
    "Now we can calculate the partial derivative with respect to $w_{11}$ at a particular coordinate:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "x = \\left( \\begin{array}{c} 1  \\\\\n",
    "                            2  \\end{array}  \\right),\\quad y = \\left( \\begin{array}{c} -3 \\\\ 3 \\end{array} \\right), then\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J}{\\partial w_{11}} &= 2y_1 x_1 \\\\\n",
    "& = 2\\times-3\\times1 \\\\\n",
    "&= \\color{red}{-6}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d333d1",
   "metadata": {
    "id": "f8d333d1"
   },
   "source": [
    "The weights $w_{11}, w_{12}, w_{21} \\text{ and } w_{22}$ are elements of the weight tensor â€“ in this case a matrix.\n",
    "\n",
    "The tensor derivative is just the **tensor of partial derivatives**.\n",
    "\n",
    "I've written it out here but you will **not ever** need to write code for this thing!\n",
    "\n",
    "That's why we have TensorFlow and other similar deep learning frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8f71ec-c90c-4906-81bb-d5fea1b1cce7",
   "metadata": {
    "id": "bb8f71ec-c90c-4906-81bb-d5fea1b1cce7"
   },
   "source": [
    "\\begin{align*}\n",
    "J &= y_1^2 + y_2^2 = (w_{11}x_1 + w_{12}x_2 + b_1)^2 + (w_{21}x_1 + w_{22}x_2  + b_2)^2\\\\\n",
    "\\\\\n",
    "\\frac{\\partial J}{\\partial w}\n",
    "&=\n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "    \\frac{\\partial J}{\\partial w_{11}} & \\frac{\\partial J}{\\partial w_{12}} \\\\\n",
    "    \\frac{\\partial J}{\\partial w_{21}} & \\frac{\\partial J}{\\partial w_{22}}\n",
    "\\end{array}\n",
    "\\right)\n",
    "            &x =\\left(\\begin{array}{c}1  \\\\2  \\end{array}  \\right),\\\\\n",
    "&=\n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "    2 y_1 \\frac{\\partial y_1}{\\partial w_{11}} & 2y_1\\frac{y_1}{\\partial w_{12}} \\\\\n",
    "    2y_2\\frac{\\partial y_2}{\\partial w_{21}}   & 2y_2\\frac{\\partial y_2}{\\partial w_{22}}\n",
    "\\end{array}\n",
    "\\right)\n",
    "=\n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "    -6 \\frac{\\partial y_1}{\\partial w_{11}} & -6\\frac{y_1}{\\partial w_{12}} \\\\\n",
    "    6 \\frac{\\partial y_2}{\\partial w_{21}} &  6\\frac{\\partial y_2}{\\partial w_{22}}\n",
    "\\end{array}\n",
    "\\right)\n",
    "            &y = \\left(\\begin{array}{c} -3 \\\\ 3\\end{array}\\right)\\\\\n",
    "&=\n",
    "\\left(\n",
    "\\begin{array}{r}\n",
    "    -6 x_1 & -6 x_2 \\\\\n",
    "    6 x_1 &  6 x_2\n",
    "\\end{array}\n",
    "\\right)\n",
    "=\n",
    "\\color{red}{\n",
    "\\left(\n",
    "\\begin{array}{r}\n",
    "    -6  & -12 \\\\\n",
    "    6  &  12\n",
    "\\end{array}\n",
    "\\right)}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c56291",
   "metadata": {
    "id": "e7c56291"
   },
   "source": [
    "The calculation is similar for the biases.\n",
    "\n",
    "This was for a system with **four** weights!\n",
    "\n",
    "Regular networks these days have **hundreds of millions** or more.\n",
    "\n",
    "This is what TensorFlow's automatic differentiation engine takes care of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f75a76e8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f75a76e8",
    "outputId": "9bf07bbf-4a64-4ca5-a084-4a8162ad9bdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      " [[1.]\n",
      " [2.]]\n",
      "\n",
      "y:\n",
      " [[-3.]\n",
      " [ 3.]]\n",
      "\n",
      "our gradient tensor:\n",
      "[[ -6. -12.]\n",
      " [  6.  12.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"x:\\n\", x.numpy())\n",
    "print()\n",
    "print(\"y:\\n\", y.numpy())\n",
    "print()\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape: # please record all operations in the \"with\" context\n",
    "    y = w @ x + b           # our operation\n",
    "    J = tf.reduce_sum(y**2) # our loss\n",
    "\n",
    "           # gradient of f w.r.t. w and b\n",
    "[df_dw, df_db] = tape.gradient(J, [w, b])      # please do all the math for us\n",
    "print(\"our gradient tensor:\")\n",
    "print(df_dw.numpy())                           # voilÃ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f67256a",
   "metadata": {
    "id": "6f67256a"
   },
   "source": [
    "##### Note\n",
    "\n",
    "We won't even need this code for most cases: it is done automatically inside `Model.fit()`.\n",
    "\n",
    "This would show up if you wanted to write your own custom loss and training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5512e4-dd79-4b28-a35a-3f5c85a5edc8",
   "metadata": {
    "id": "1e31f2b4-7101-49c5-b893-df3c8b1c2ab5"
   },
   "source": [
    "## Next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1894babb-84b8-4ca2-90f9-9d78d8a4ac0a",
   "metadata": {
    "id": "1e31f2b4-7101-49c5-b893-df3c8b1c2ab5"
   },
   "source": [
    "1. Derivatives  \n",
    "2. Derivatives for tensors: the gradient\n",
    "3. **Optimization: stochastic gradient descent, momentum**\n",
    "4. Chaining derivatives: the backpropagation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cad74b-00c2-4d3c-8a69-b28eec81a7df",
   "metadata": {
    "id": "a65cc41f-f8df-4ea8-96ff-7db6d8d99b4f"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d02a8f5-81fb-49d0-93db-99dbf64007ab",
   "metadata": {
    "id": "a65cc41f-f8df-4ea8-96ff-7db6d8d99b4f"
   },
   "source": [
    "## Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55b0acf",
   "metadata": {
    "id": "d55b0acf"
   },
   "source": [
    "The gradient tensor is written $\\nabla J$ (the nabla symbol).\n",
    "\n",
    "The loss function is at a minimum when $w$ and $b$ are solutions of $\\nabla J = 0$.\n",
    "\n",
    "However, computing all the solutions explicitly is intractable, for two reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56965bf9-f780-41e2-b739-63db9ae4aa94",
   "metadata": {
    "id": "56965bf9-f780-41e2-b739-63db9ae4aa94"
   },
   "source": [
    "First, because of the function itself, **too many parameters**!\n",
    "\n",
    "It makes knowing the behaviour of the function very difficult.\n",
    "\n",
    "This includes knowing **whether a minimum is local or global**.\n",
    "\n",
    "Let's call this **the landscape problem**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcddcd2d",
   "metadata": {
    "id": "fcddcd2d"
   },
   "outputs": [],
   "source": [
    "def f(x, x_1, x_2, x_3, x_4):\n",
    "    return (x - x_1) * (x - x_2) * (x - x_3) * (x - x_4)\n",
    "\n",
    "def plot_loss():\n",
    "    x = np.linspace(0, 10, num=101)\n",
    "    x_1, x_2, x_3, x_4 = 0, 4, 7, 8\n",
    "    y = f(x, x_1, x_2, x_3, x_4)\n",
    "\n",
    "    plt.plot(x, y + 150)\n",
    "    plt.figtext(0.2, 0.25, 'Global\\nminimum', fontsize='large')\n",
    "    plt.figtext(0.6, 0.4, 'Local\\nminimum', fontsize='large')\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    ax.xaxis.set_ticks_position('bottom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "132a0feb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "132a0feb",
    "outputId": "946d9acd-f215-4348-e808-3adac44ab81c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGdCAYAAAA44ojeAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABQnUlEQVR4nO3dd3hUZd7G8e+k9wkJKYQQOoHQa4iIqLRFcEXRBRcVFetiZWWVfVV41RVk17K62BHwVSy4YkFFESmLhBZEeg8klCRAyqSQNnPeP0JmjaASSHJmJvfnuuaSOefMmd8ZIXPnOU+xGIZhICIiIuJCvMwuQEREROTnFFBERETE5SigiIiIiMtRQBERERGXo4AiIiIiLkcBRURERFyOAoqIiIi4HAUUERERcTluGVAMw8Bms6E55kRERDyTWwaUwsJCrFYrhYWFZpciIiIi9cAtA4qIiIh4NgUUERERcTkKKCIiIuJyFFBERETE5SigiIiIiMtRQBERERGXo4AiIiIiLkcBRURERFyOAoqIiIi4HAUUERERcTkKKCIiIuJyFFBERETE5SigiIiIiMtRQBERERGXo4AiIiIiTmv2neB/Fm1lybZjptahgCIiIiJOqQdO8u66DFbsPm5qHQooIiIi4rQrqxCAxNhQU+tQQBERERGnPdmnA0qMGwWU6dOnY7FYajw6duzo3F9aWsqkSZOIjIwkJCSEMWPGkJ2dXeMcGRkZjBw5kqCgIKKjo5kyZQqVlZV1czUiIiJy3krKK8nILQHMb0Hxqe0LOnfuzLfffvvfE/j89xQPPvggX3zxBQsXLsRqtXLPPfdwzTXX8P333wNgt9sZOXIksbGxrFmzhmPHjnHTTTfh6+vL008/XQeXIyIiIudrb3YRhgFNQ/yIDPE3tZZaBxQfHx9iY2PP2F5QUMCcOXNYsGABl19+OQBz586lU6dOrF27lv79+/PNN9+wY8cOvv32W2JiYujRowdPPvkkDz/8MNOnT8fPz+/Cr0hERETOy24X6X8C59EHZe/evcTFxdGmTRvGjx9PRkYGAGlpaVRUVDBkyBDnsR07diQhIYHU1FQAUlNT6dq1KzExMc5jhg8fjs1mY/v27b/4nmVlZdhsthoPERERqVu7T/c/6WBy/xOoZUBJTk5m3rx5LFmyhFdeeYX09HQGDhxIYWEhWVlZ+Pn5ER4eXuM1MTExZGVlAZCVlVUjnFTvr973S2bMmIHVanU+WrRoUZuyRURE5BxUt6B0dIEWlFrd4hkxYoTzz926dSM5OZmWLVvy4YcfEhgYWOfFVZs6dSqTJ092PrfZbAopIiIidcxtW1B+Ljw8nA4dOrBv3z5iY2MpLy8nPz+/xjHZ2dnOPiuxsbFnjOqpfn62fi3V/P39CQsLq/EQERGRupNbXM7xwjLAAwJKUVER+/fvp1mzZvTu3RtfX1+WLVvm3L97924yMjJISUkBICUlha1bt5KTk+M8ZunSpYSFhZGUlHQhpYiIiMgFqL690yIikGD/Wo+hqXO1quChhx7iyiuvpGXLlhw9epRp06bh7e3N9ddfj9VqZeLEiUyePJmIiAjCwsK49957SUlJoX///gAMGzaMpKQkbrzxRmbNmkVWVhaPPvookyZNwt/f3OFMIiIijdnurKoBKIkxrnGXolYB5fDhw1x//fWcPHmSqKgoLr74YtauXUtUVBQAzz//PF5eXowZM4aysjKGDx/Oyy+/7Hy9t7c3ixcv5u677yYlJYXg4GAmTJjAE088UbdXJSIiIrWyO7sIcI0OsgAWwzAMs4uoLZvNhtVqpaCgQP1RRERE6sA1L3/Ppox8Xry+J7/vHmd2OVqLR0REpLEzDIM9LtaCooAiIiLSyB3JP0VRWSW+3hZaNw02uxxAAUVERKTRq17BuG1UCL7erhENXKMKERERMc2uLNeZoK2aAoqIiEgjt8eFFgmspoAiIiLSyFW3oCSqBUVERERcQYXdwYHjxYBaUERERMRFHDxRTLndQbCfN83D62/h39pSQBEREWnEnCsYx4bi5WUxuZr/UkARERFpxKoXCXSVCdqqKaCIiIg0YrtdcIgxKKCIiIg0atW3eFypgywooIiIiDRaxWWVZOSWAK41xBgUUERERBqtXVmFGAbEhPkTGeJvdjk1KKCIiIg0UjuO2QBIahZmciVnUkARERFppHYcPR1Q4hRQRERExEX8twXFanIlZ1JAERERaYQq7Q52HVMLioiIiLiQgyeLKat0EOTnTcuIILPLOYMCioiISCO0/XT/k07NwlxqivtqCigiIiKNkCuP4AEFFBERkUapegRPZxfsfwIKKCIiIo2OYRguPcQYFFBEREQanZzCMk4Wl+PtZXG5RQKrKaCIiIg0MtWtJ22jggnw9Ta5mrNTQBEREWlkXL2DLCigiIiINDqu3v8EFFBEREQaHVee4r6aAoqIiEgjUlRWycGTxQB0auaaHWRBAUVERKRR2Z1lwzAgNiyAyBB/s8v5RQooIiIijYg79D8BBRQREZFGxR1G8IACioiISKOiFhQRERFxKZV2B7uyCgG1oIiIiIiLSD9RTFmlg2A/bxIigswu51cpoIiIiDQS1f1POjULw8vLYnI1v04BRUREpJHYdqQAcP3+J6CAIiIi0mhsPR1QujZ33RlkqymgiIiINAIOh8G2I1W3eLrGK6CIiIiIC0g/WUxRWSUBvl60iwoxu5zfpIAiIiLSCGw9XHV7p3OcFR9v1//6d/0KRURE5IJtOew+/U9AAUVERKRR2HokH4BubtD/BBRQREREPJ79Jx1kFVBERETEJew/XsSpCjvBft60bur6HWRBAUVERMTjVfc/6dzcireLzyBbTQFFRETEw209nA9ANzfpIAsKKCIiIh5vS/UMsm7S/wQUUERERDxahd3BjqPVHWTDzS2mFhRQREREPNje7CLKKh2EBvjQMiLI7HLOmQKKiIiIB6ue/6RrcytebtJBFhRQREREPJpzBlk36n8CCigiIiIebevpDrLdmoebW0gtKaCIiIh4qLJKOzuPudcMstUUUERERDzUnqwiKuwG4UG+xDcJNLucWlFAERER8VBbftJB1mJxnw6yoIAiIiLisbae7iDrbrd34AIDysyZM7FYLDzwwAPObaWlpUyaNInIyEhCQkIYM2YM2dnZNV6XkZHByJEjCQoKIjo6milTplBZWXkhpYiIiMjPOEfwuFkHWbiAgLJhwwZee+01unXrVmP7gw8+yOeff87ChQtZuXIlR48e5ZprrnHut9vtjBw5kvLyctasWcP8+fOZN28ejz/++PlfhYiIiNRQWmFnT3Yh4H5DjOE8A0pRURHjx4/njTfeoEmTJs7tBQUFzJkzh+eee47LL7+c3r17M3fuXNasWcPatWsB+Oabb9ixYwfvvPMOPXr0YMSIETz55JPMnj2b8vLyurkqERGRRm7rkQIqHQZRof7EWQPMLqfWziugTJo0iZEjRzJkyJAa29PS0qioqKixvWPHjiQkJJCamgpAamoqXbt2JSYmxnnM8OHDsdlsbN++/azvV1ZWhs1mq/EQERGRX/ZDRh4AvRLC3a6DLIBPbV/w/vvvs2nTJjZs2HDGvqysLPz8/AgPD6+xPSYmhqysLOcxPw0n1fur953NjBkz+N///d/alioiItJobTqUD0DPhCa/fqCLqlULSmZmJvfffz/vvvsuAQEN11w0depUCgoKnI/MzMwGe28RERF3YxgGm063oPRsEW5uMeepVgElLS2NnJwcevXqhY+PDz4+PqxcuZIXX3wRHx8fYmJiKC8vJz8/v8brsrOziY2NBSA2NvaMUT3Vz6uP+Tl/f3/CwsJqPEREROTsjhWUklNYhreXhW7x4WaXc15qFVAGDx7M1q1b2bx5s/PRp08fxo8f7/yzr68vy5Ytc75m9+7dZGRkkJKSAkBKSgpbt24lJyfHeczSpUsJCwsjKSmpji5LRESk8apuPenULJRAP2+Tqzk/teqDEhoaSpcuXWpsCw4OJjIy0rl94sSJTJ48mYiICMLCwrj33ntJSUmhf//+AAwbNoykpCRuvPFGZs2aRVZWFo8++iiTJk3C39+/ji5LRESk8fohIx+Ani3cs/8JnEcn2d/y/PPP4+XlxZgxYygrK2P48OG8/PLLzv3e3t4sXryYu+++m5SUFIKDg5kwYQJPPPFEXZciIiLSKDlH8LQMN7eQC2AxDMMwu4jastlsWK1WCgoK1B9FRETkJ8oq7XSd9g3ldgcrHrqUVk2DzS7pvGgtHhEREQ+y46iNcruDiGA/WkYGmV3OeVNAERER8SCbnP1P3HOCtmoKKCIiIh6kuv9Jz4Rwcwu5QAooIiIiHqR6BE8vN51BtpoCioiIiIfIsZVyJP8UFgt0c9MZZKspoIiIiHiI6v4niTGhhPjX+UwiDUoBRURExEP8kFnd/8S9b++AAoqIiIjH+MG5gnG4qXXUBQUUERERD1Bhd7DlSD4AvRRQRERExBXsziqktMJBWIAPbZqGmF3OBVNAERER8QDVKxj3SGiCl5f7TtBWTQFFRETEA2w8eHqBQA+4vQMKKCIiIm7PMAzWp+cC0K9VhMnV1A0FFBERETd3OO8UWbZSfLwsHjHEGBRQRERE3F5160nXeCuBft4mV1M3FFBERETcnKfd3gEFFBEREbe34eDpgNJaAUVERERcwPHCMg6cKMZigT4tFVBERETEBWw83XqSGBOKNcjX5GrqjgKKiIiIG1uX7nm3d0ABRURExK1V9z/p60EdZEEBRURExG3ZSivYecwGqAVFREREXETaoTwcBrSMDCImLMDscuqUAoqIiIib2pDumbd3QAFFRETEbXni/CfVFFBERETcUGmFnR8zCwDPmkG2mgKKiIiIG/oxM59yu4OoUH9aRgaZXU6dU0ARERFxQz+9vWOxWEyupu4poIiIiLihdR64QOBPKaCIiIi4mUq7g02H8gDPHMEDCigiIiJuZ8uRAorL7VgDfUmMDTW7nHqhgCIiIuJm1uw7AUBKm0i8vTyv/wkooIiIiLid7/edBGBAu0iTK6k/CigiIiJupLTCTlpGVf+Ti9o1Nbma+qOAIiIi4kY2HsyjvNJBbFgAbZoGm11OvVFAERERcSPf76/qf3JRu0iPnP+kmgKKiIiIG6nuIHtRW8+9vQMKKCIiIm6j4FQFW49Urb/jyR1kQQFFRETEbaw9cBKHAW2aBtPMGmh2OfVKAUVERMRNpO6vGl58kYe3noACioiIiNv4/nT/kwEe3v8EFFBERETcQo6tlL05RVgskNJWLSgiIiLiAtacvr3TOS6M8CA/k6upfwooIiIibqAx3d4BBRQRERGXZxiGswXFk6e3/ykFFBERERd36GQJR/JP4ettoW+rJmaX0yAUUERERFxc9fT2PVs0IcjPx+RqGoYCioiIiItbtec4AAMaye0dUEARERFxaeWVDr7fV9X/5NLEKJOraTgKKCIiIi4s7VAeRWWVRAb70bW51exyGowCioiIiAtbsScHgEEdovDysphcTcNRQBEREXFhK3ZV9T8Z1Ihu74ACioiIiMs6mn+K3dmFeFngkvYKKCIiIuICVp4evdOjRThNgj1/evufUkARERFxUSt2V/U/uTQx2uRKGp4CioiIiAsqr3Swem/VBG2NaXhxNQUUERERF7TxUC7F5XaahvjRJa7xDC+uVquA8sorr9CtWzfCwsIICwsjJSWFr776yrm/tLSUSZMmERkZSUhICGPGjCE7O7vGOTIyMhg5ciRBQUFER0czZcoUKisr6+ZqREREPMTK3VX9Ty5pZMOLq9UqoMTHxzNz5kzS0tLYuHEjl19+OVdddRXbt28H4MEHH+Tzzz9n4cKFrFy5kqNHj3LNNdc4X2+32xk5ciTl5eWsWbOG+fPnM2/ePB5//PG6vSoRERE3t+J0QLmsEfY/AbAYhmFcyAkiIiL4+9//zrXXXktUVBQLFizg2muvBWDXrl106tSJ1NRU+vfvz1dffcWoUaM4evQoMTExALz66qs8/PDDHD9+HD+/c+uhbLPZsFqtFBQUEBYWdiHli4iIuJyj+ae4aOZ3eFlg02NDCQ9qXCN44AL6oNjtdt5//32Ki4tJSUkhLS2NiooKhgwZ4jymY8eOJCQkkJqaCkBqaipdu3Z1hhOA4cOHY7PZnK0wZ1NWVobNZqvxEBER8VTVrSc9E5o0ynAC5xFQtm7dSkhICP7+/tx1110sWrSIpKQksrKy8PPzIzw8vMbxMTExZGVlAZCVlVUjnFTvr973S2bMmIHVanU+WrRoUduyRURE3Eb18OLLGuHonWq1DiiJiYls3ryZdevWcffddzNhwgR27NhRH7U5TZ06lYKCAucjMzOzXt9PRETELFWrF1cPL26c/U8AfGr7Aj8/P9q1awdA79692bBhA//85z8ZO3Ys5eXl5Ofn12hFyc7OJjY2FoDY2FjWr19f43zVo3yqjzkbf39//P39a1uqiIiI21mz/wTF5XaiQ/1JatZ4+1le8DwoDoeDsrIyevfuja+vL8uWLXPu2717NxkZGaSkpACQkpLC1q1bycnJcR6zdOlSwsLCSEpKutBSRERE3N7X26t+cR/WOaZRDi+uVqsWlKlTpzJixAgSEhIoLCxkwYIFrFixgq+//hqr1crEiROZPHkyERERhIWFce+995KSkkL//v0BGDZsGElJSdx4443MmjWLrKwsHn30USZNmqQWEhERafQcDoOlO04HlKRfvrPQGNQqoOTk5HDTTTdx7NgxrFYr3bp14+uvv2bo0KEAPP/883h5eTFmzBjKysoYPnw4L7/8svP13t7eLF68mLvvvpuUlBSCg4OZMGECTzzxRN1elYiIiBv6ITOPE0VlhAb40L9NpNnlmOqC50Exg+ZBERERT/T0lzt5fdUBRveI44VxPc0ux1Rai0dERMQFGIbB19urptwY1rlx394BBRQRERGXsCe7iEMnS/Dz8WJQh8Y7/0k1BRQREREXUN16MrBdU4L9az0LiMdRQBEREXEB1QFluG7vAAooIiIipjucV8L2oza8LDC4U+OdPfanFFBERERM9s3pydn6tIogMkTzgoECioiIiOmco3eSYn7jyMZDAUVERMREucXlbDiYC6j/yU8poIiIiJjo253ZOAzo1CyMFhFBZpfjMhRQRERETPTl1mMADO+s2zs/pYAiIiJiktziclbvPQHAld3jTK7GtSigiIiImOTLrceodBh0jgujbVSI2eW4FAUUERERk3z241EAfq/WkzMooIiIiJjgWMEp5+idUQooZ1BAERERMcHiH49hGNC3VROahweaXY7LUUARERExgW7v/DoFFBERkQaWfqKYrUcK8PaycEXXZmaX45IUUERERBrYZ5urWk8GtGuqtXd+gQKKiIhIAzIMg89+PALo9s6vUUARERFpQDuO2dh/vBg/Hy/NHvsrFFBEREQaUHXn2MsTowkN8DW5GtelgCIiItJAHA6DxT9Wrb3z+x66vfNrFFBEREQayNr0kxzJP0WIvw+Xd4w2uxyXpoAiIiLSQBZuPAxULQwY4OttcjWuTQFFRESkARScquDLrVW3d8b2bWFyNa5PAUVERKQBfPbjUcoqHSTGhNI93mp2OS5PAUVERKQBfLghE4Dr+sRjsVhMrsb1KaCIiIjUsx1HbWw9UoCvt4VresWbXY5bUEARERGpZx9urGo9GZoUQ0Swn8nVuAcFFBERkXpUVmnnk81VU9v/oY86x54rBRQREanBYrEwffp0s8vwGN9szya/pIJm1gAGto8yuxy3oYAiImKSefPmYbFY2Lhxo9mlSD2qvr1zbe94vL3UOfZcKaCIiIjUk8N5JazedwKA63rr9k5tKKCIiIjUk4UbD2MYcFHbSBIig8wux60ooIiIuLAffviBESNGEBYWRkhICIMHD2bt2rVnHJefn8+DDz5Iq1at8Pf3Jz4+nptuuokTJ6p+ey8vL+fxxx+nd+/eWK1WgoODGThwIMuXL2/oS2o0yisdLFifAcC4fgkmV+N+fMwuQEREzm779u0MHDiQsLAw/vKXv+Dr68trr73GpZdeysqVK0lOTgagqKiIgQMHsnPnTm699VZ69erFiRMn+Oyzzzh8+DBNmzbFZrPx5ptvcv3113P77bdTWFjInDlzGD58OOvXr6dHjx7mXqwH+mrbMY4XlhET5s+ILrFml+N2FFBERFzUo48+SkVFBatXr6ZNmzYA3HTTTSQmJvKXv/yFlStXAvD3v/+dbdu28fHHH3P11VfXeL1hGAA0adKEgwcP4uf33zk4br/9djp27MhLL73EnDlzGvDKGoe3vj8IwA3JLfH11g2L2tInJiLigux2O9988w2jR492hhOAZs2a8cc//pHVq1djs9kA+Pe//0337t1rhJNq1VOqe3t7O8OJw+EgNzeXyspK+vTpw6ZNmxrgihqXHzLy+DEzHz9vL65P1u2d86GAIiLigo4fP05JSQmJiYln7OvUqRMOh4PMzKrhq/v376dLly6/ec758+fTrVs3AgICiIyMJCoqii+++IKCgoI6r7+xm7fmIABXdo+jaYi/ucW4KQUUEZFG4J133uHmm2+mbdu2zJkzhyVLlrB06VIuv/xyHA6H2eV5lGxbKV9sOQbALQNamVuMG1MfFBERFxQVFUVQUBC7d+8+Y9+uXbvw8vKiRYuqeTXatm3Ltm3bfvV8H330EW3atOHjjz+usZLutGnT6rZw4d11GVQ6DPq2akKX5lazy3FbakEREXFB3t7eDBs2jE8//ZSDBw86t2dnZ7NgwQIuvvhiwsLCABgzZgw//vgjixYtOuM81Z1kvb29azwHWLduHampqfV4FY1PWaWdBesOAXDzRa1Nrsa9qQVFRMRkb731FkuWLDlj+/Tp01m6dCkXX3wxf/rTn/Dx8eG1116jrKyMWbNmOY+bMmUKH330Eddddx233norvXv3Jjc3l88++4xXX32V7t27M2rUKOcon5EjR5Kens6rr75KUlISRUVFDXm5Hu2LLcc4UVROM2sAwzrHmF2OW1NAEREx2SuvvHLW7TfffDP/+c9/mDp1KjNmzMDhcJCcnMw777zjnAMFICQkhP/85z9MmzaNRYsWMX/+fKKjoxk8eDDx8fHOc2VlZfHaa6/x9ddfk5SUxDvvvMPChQtZsWJFQ1ymxzMMw9k59ob+Glp8oSzGT9v73ITNZsNqtVJQUOBs4hQRETHTmv0n+OMb6/D38SJ16mAigv1++0XyixTvRERE6sDs5fsAGNu3hcJJHVBAERERuUBph/L4ft9JfLws3DmordnleAQFFBERkQtU3XpyTa/mNA8PNLkaz6CAIiIicgG2HSngu105eFng7kvbmV2Ox1BAERERuQAvr6hqPRnVLY7WTYNNrsZzKKCIiLihgwcPYrFYmDdv3nm93mKxMH369DqtqTHal1PIV9uyAJh0mVpP6pICioiIyHl6efl+DAOGJcWQGBtqdjkeRRO1iYi4oZYtW3Lq1Cl8fX3P6/WnTp3Cx0dfARci42QJn/54FIB7LlfrSV3T304RETdksVgICAg479dfyGulyr+W78XuMLikQxTd4sPNLsfj6BaPiIhJpk+fjsViYc+ePdxwww1YrVaioqJ47LHHMAyDzMxMrrrqKsLCwoiNjeXZZ591vvZsfVBuvvlmQkJCOHLkCKNHjyYkJISoqCgeeugh7HZ7jff+eR+UC6kFYN68eVgslhoLGwKsWLECi8VSYzr9Sy+9lC5durBlyxYGDRpEUFAQ7dq146OPPgJg5cqVJCcnExgYSGJiIt9+++2FfdD1YHdWIR+lHQbg/sHtTa7GMymgiIiYbOzYsTgcDmbOnElycjJPPfUUL7zwAkOHDqV58+Y888wztGvXjoceeohVq1b96rnsdjvDhw8nMjKSf/zjHwwaNIhnn32W119/vcFr+TV5eXmMGjWK5ORkZs2ahb+/P+PGjeODDz5g3LhxXHHFFcycOZPi4mKuvfZaCgsLz/u96sMzS3bhMGBEl1h6t2xidjmeyXBDBQUFBmAUFBSYXYqIyHmbNm2aARh33HGHc1tlZaURHx9vWCwWY+bMmc7teXl5RmBgoDFhwgTDMAwjPT3dAIy5c+c6j5kwYYIBGE888USN9+nZs6fRu3fvGtsAY9q0aXVSi2EYxty5cw3ASE9Pr/E+y5cvNwBj+fLlzm2DBg0yAGPBggXObbt27TIAw8vLy1i7dq1z+9dff33GdZptzb4TRsuHFxttp35h7M8pNLscj1WrFpQZM2bQt29fQkNDiY6OZvTo0ezevbvGMaWlpUyaNInIyEhCQkIYM2YM2dnZNY7JyMhg5MiRBAUFER0dzZQpU6isrDzPiCUi4t5uu+0255+9vb3p06cPhmEwceJE5/bw8HASExM5cODAb57vrrvuqvF84MCB5/S6+qjll4SEhDBu3Djn88TERMLDw+nUqVONlZqr/3wh71WXHA6DGV/tBOCPyQm0iQoxuSLPVauAsnLlSiZNmsTatWtZunQpFRUVDBs2jOLiYucxDz74IJ9//jkLFy5k5cqVHD16lGuuuca53263M3LkSMrLy1mzZg3z589n3rx5PP7443V3VSIibiQhIaHGc6vVSkBAAE2bNj1je15e3q+eKyAggKioqBrbmjRp8puvq49afk18fDwWi+WMc7Zo0eKMbcAFvVdd+mLrMbYcLiDYz5v71PekXtVqFM+SJUtqPJ83bx7R0dGkpaVxySWXUFBQwJw5c1iwYAGXX345AHPnzqVTp06sXbuW/v37880337Bjxw6+/fZbYmJi6NGjB08++SQPP/ww06dPx89PK0CKSOPi7e19TtsADMOo9bkaopafh41qP++c+1vnPN/rbgjllQ7+/nXVXYM7B7WlaYi/yRV5tgvqJFtQUABAREQEAGlpaVRUVDBkyBDnMR07diQhIYHU1FQAUlNT6dq1KzExMc5jhg8fjs1mY/v27Wd9n7KyMmw2W42HiIi4jiZNqjqK5ufn19h+6NAhE6qpH++uO0RGbglRof7cNrC12eV4vPMOKA6HgwceeIABAwbQpUsXALKysvDz8yM8PLzGsTExMWRlZTmP+Wk4qd5fve9sZsyYgdVqdT5+3gQoIiLmatu2LUCNkT12u/2cRw+5uoKSCl76rmrNnclDOxDkp2nE6tt5f8KTJk1i27ZtrF69ui7rOaupU6cyefJk53ObzaaQIiLiQjp37kz//v2ZOnUqubm5RERE8P7773vMAIiZS3aRW1xO++gQrusdb3Y5jcJ5BZR77rmHxYsXs2rVKuLj//s/KjY2lvLycvLz82u0omRnZxMbG+s8Zv369TXOVz3Kp/qYn/P398ffX/f6RERc2bvvvsudd97JzJkzCQ8PZ+LEiVx22WUMHTrU7NIuSNqhXN5bnwHA367uio+3phBrCBajFj2PDMPg3nvvZdGiRaxYsYL27Wv2YC4oKCAqKor33nuPMWPGALB79246duxIamoq/fv356uvvmLUqFEcO3aM6OhoAF5//XWmTJlCTk7OOQURm82G1WqloKCAsLCw2lyviIjIOauwOxj14mp2Zxfyhz7xzLq2u9klNRq1akGZNGkSCxYs4NNPPyU0NNTZZ8RqtRIYGIjVamXixIlMnjyZiIgIwsLCuPfee0lJSaF///4ADBs2jKSkJG688UZmzZpFVlYWjz76KJMmTVIriYiIuJQ5q9PZnV1IRLAfU0d0MrucRqVWLSi/NIxs7ty53HzzzUDVRG1//vOfee+99ygrK2P48OG8/PLLNW7fHDp0iLvvvpsVK1YQHBzMhAkTmDlz5jmvrKkWFBERqW+ZuSUMfX4lpRUO/nFdd65V35MGVauA4ioUUEREpD4ZhsHE+Rv5blcO/dtE8N7t/X/xl3SpH+rpIyIi8jNfbcviu105+HpbeGp0V4UTEyigiIiI/ES2rZT/WbQVgLsHtaVdtNbbMYMCioiIyGkOh8HkDzeTV1JB57gw7rlc6+2YRQFFRETktDdXH+D7fScJ8PXin+N64uejr0mzaK5eEXF5hmFQXG4nt6ic3JJycovLKCytpKzSQYXdQfnp/3p7eeHn44W/txe+PhaC/HxoEuRHRLAvTYL8CA/yw9tLfQnk7LYdKXAuBvj4qM66tWMyBRQRcRn5JeVsP2pjX04Rh06WkJFbzKGTJWTmlVBa4bjg81ssEBMaQHyTwNOPIBIigugQG0r76BCC/fUjsbEqKa/kvvd/oMJuMLxzDNf303IqZtO/RhExRVmlnR8zC1iffpJtR2xsO1rA4bxTv/qaAF8vIoP9aRLsS1iAL34+Xvh5ezn/W+kwKK90UH66VaWorJL8knLySiooOFWBYUCWrZQsWykbD+Wdcf74JoEkxoTSubmVni3C6d4inIhgv/r6CMSFPLl4JweOFxMT5s/Ma7pp1I4LUEARkQbhcBhsOVLA6r3HST1wkrRDeWdtFUmICCIxNpRWkUEkRAbTMqKqlSM6zP+CVpCttDvILSnnWH4ph/NOcTivqmUm/UQxe7KLOF5Ydnr7KZbtyqlRT8+EcPq3iaR/m0haRQbpy8vDLNyY6Vxr57k/9KCJQqlL0ERtIlJvyirtpO4/ydId2SzdkU1OYVmN/U1D/EhuHUnPhHCS4sLo3MyKNcjXlFpzi8vZk13I7qxCfjycz+bMfA4cLz7juJgwf/q3ieTidk0ZlBhFdGiACdVKXdl4MJfr31hLhd3gvsvbMXlYotklyWkKKCJSpxwOg/UHc/l402G+2ppFYVmlc1+wnzcD20dxUbtIUtpE0i46xKVbIwpOVbDlcD4bD+aReuAkmzPyKbfXbPXpHBfGpYlRXJYYTc+EJuqE60Yyc0sYPft7ThaXM6JLLLP/2Asv/f9zGQooIlInMk6W8FFaJh//cKRGX5KoUH+GdIphWOcYLmobib+Pt4lVXpjSCjubMvJYs+8kK/ccZ+uRghr7m4b4MTQphmGdY93+Wj1dcVklY15Zw66sQjrHhbHwrpQLuoUodU8BRUTOm2EY/GfvCeavOch3u3Oo/mkS4u/DyK7NuLpXc/q1ivDY30qPF5axas9xVuw5zordORSW/re1KMTfhyGdohnVLY6BHZoqrLgQh8PgznfSWLojm6Yh/nx2zwDiwgPNLkt+RgFFRGqtpLySj9IOM2/NwRr9NAa2b8q1veMZlhRLoF/j+kIur3Sw9sBJvt6edUZ/m9AAH4Z3juXK7nEMaBuJj7cm/zKLYRg8sXgHc78/iJ+PFx/c0Z+eCU3MLkvOQgFFRM5ZYWkFb6ceYs7qdHKLy4GqloJre8dzU0pL2kRpYiuo+g39h8w8Fm85xhdbjtUIK1Gh/vy+exxX92xO57gwl+6D42kMw2Dmkl28tvIAAP8c14OrejQ3uSr5JQooIvKb8kvKeev7g8z7Ph3b6dsYCRFBTLy4Ndf0ak5ogDkjb9yB3WGw4WAun/94lC+3HiOvpMK5r310CNf0iueaXs2JCdNooPr27De7eem7fQA8NboLN/RvaXJF8msUUETkF50qt/PW9+m8umK/czRO26hg7rm8HVd2i9Otiloqr3Swas9xFm0+wtId2ZRXVo0I8rLAJR2iuLZ3PEM6xRDg27hujzWEf367l+e/3QPA9CuTuHlAa5Mrkt+igCIiZ6i0O1iYdpgXvt1Dtq3q9kTH2FDuG9ye33WO9dhOrw3JVlrBl1uO8e9Nh9lw8L+z2loDfRndI44/9G1B5ziriRV6jtnL9znX2Hl0ZCduG9jG5IrkXCigiEgNy3fn8NTiHew/3fk1vkkgU4YncmW3OAWTepJ+oph/px3m35sOc6yg1Lm9c1wYY/u24KruzU2bwM6d2R0GTy7ewbw1BwF4+HcdufvStuYWJedMAUVEgKpJq55YvIOlO7IBaBLky72Xt2d8/wQNkW0gdofB9/tO8MHGTJZuz3ZOCufv48WILrH8oW8L+reOVFA8B0Vlldz33g98d3rZgqkjOnLnIIUTd6KAItLIlVbYeWXFfl5ZuZ/ySgc+XhZuGdCKewe3J0ydX02TV1zOJ5uP8MGGTHZlFTq3J0QE8Yc+8VzbuwWxVnWsPZuj+ae4dd4GdmUV4u/jxfNje3BF12ZmlyW1pIAi0oh9v+8EUz/eSkZuCQAD2kUy/crOtI8JNbkyqWYYBlsOF/DBxkw+23yUotOdlb0sMKhDFGP7tuDyjjH4+ajDMkDaoTzueieN44VlNA3x580JfejRItzssuQ8KKCINEIFpyp4+oudfLAxE4Bm1gAeHZnEFV1jNS+HCyspr+SLLcdYuPEw6w/mOrdHBvsxumdzru0dT6dmjfNnYoXdwUvf7WP28n3YHQaJMaHMubkP8U2CzC5NzpMCikgjs2RbFo9/us05ediElJZM+V1HQvy1Dok7OXC8iIVph/l32uEaE8F1jgtjTK94ruoRR2SIv4kVNpx9OUVM/nAzWw5XrY30++5x/O3qLpqfx80poIg0EgUlFTz+2TY+3XwUgDZRwTwzpht9W0WYXJlciEq7g1V7j/NR2mG+3ZHj7Fjr42VhUIcorurZnKGdYjxy6YFKu4N31h5i5pJdlFY4CAvw4amru/L77nFmlyZ1QAFFpBFYs+8Ef174I8cKSvH2snDHJW24f3B7TQjmYfKKy/l8y1EWbjxcY6XlYD9vhnepXguoqdv3VzEMg+W7c5jx5S725hQBVetA/f3a7uo47EEUUH6mtMJOaYWd8CC/Oj2viBlKK+z84+vdvLk6HYBWkUE8N7YHvbQ4msfbl1PIJz8c5ZPNRzicd8q53Rroy9CkGEZ2bcaAdu4XVrYeLuDpL3eSeuAkAOFBvvx5aAfGJ7fU8GsPo4DyE2+nHuSZr3Yxrl8Cj41KqrPziphhX04R9yzY5Byien2/BB4d2Ylg9TVpVAzDIO1QHp9sPsKSbVmcKCp37gsN8OHSxGgGd4zm0sQol/3FzOEw+M++E/xf6kG+3Vk1r4mfjxe3XNSKP13aTpPYeSgFlJ9Ysi2Lu95Jo0VEIKumXKbRDOK2Pt50mEc/2UZJuZ3IYD+eGdONIUkxZpclJrM7DNan5/Ll1mN8tS2LE0X/7VzrZYE+LSMYlBhFSttIujW3mr7WUkFJBQvTMnln7SEOnixxbr+6Z3P+PKyDRuh4OAWUnygpr6TnE0spq3Tw1f0DG+1wPXFfp8rtTPtsGx9uPAzARW0jeWFcD6JDdV9earI7DDZn5rFsZw7f7cqpMRkcQIi/D/1aR9C/TQTd48Pp0tzaIK1vh04WO2tal36SCnvVV1RogA/X9o7nhv4taRsVUu91iPkUUH7mtvkb+XZnNg8O6cD9Q9rX6blF6tO+nCL+9G4ae7KLsFjg/sHtuffy9njrvrycg8zcElbszuH7fSdZm36S/JKKGvu9LNAuOoRu8eF0jA2lddNgWjcNpkVEEL7n2dJiK61g17FCdh6zseOojY2Hcp1rQFXrGBvKTSmtGN0zjiA/3Z5sTBRQfubDjZn85aMtdI4L44v7BtbpuUXqy5JtWTy08EeKyiqJCvXnn+N6cFHbpmaXJW7K4TDYmWUjdf9JNhzMZcvhghqLGP6Ut5eFuPAAIoP9iQj2cz78f9b5ttzuILeonBNFZZwoKud4YRlZtjPP6eNloW+rCAZ3iubyjtG0UWtJo6WA8jMni8ro+7dvcRiw+uHLdI9TXJrdYfDc0t3MXr4fgOTWEfzrj72ICm0cE3RJw8mxlbLlcAFbjhSwP6eIAyeKOXiimFMV9gs6b5w1gKS4MDo1C6NznJWUtpFYA9XpVUDtZT8TGeJPn1YRrE/PZemObG4Z0NrskkTOKr+knPve38yqPccBuHVAa6Ze0fG8m9tFfk10WABDkgJqdLY2DINsWxmZeSXkFpeTV1xObknVf6v7jlTzsliIDPGjaYgfkcH+RIb40SoymCbBrjlySMyngHIWw5JiFFDEpe3NLmTi/I1k5JYQ4OvFM2O6cVWP5maXJY2MxWIh1hqgydGkXuhXrbMYlhQLwLr0XPJLyn/j6IY1ffr08x7+3KpVK0aNGlVntRw8eBCLxcK8efPq7Jzy25bvzuGal9eQkVtCi4hAPr57gMKJiHgcBZSzSIgMomNsKHaHwXe7chrkPdPT07nnnnvo0KEDQUFBBAUFkZSUxKRJk9iyZUuD1CCuzTAM3vzPASbO20BhWSX9Wkfw6aSLSYrTcHgR8Ty6xfMLhiXFsCurkG+2Z3NNr/h6fa/FixczduxYfHx8GD9+PN27d8fLy4tdu3bx8ccf88orr5Cenk7Lli3rtQ5xXeWVDqZ9to331mcCMLZPC54c3cXtpikXETlXCii/YFjnWF78bh8r9xyntMJeb4uq7d+/n3HjxtGyZUuWLVtGs2bNaux/5plnePnll/Hy0hdRY1VwqoK730ljzf6TeFngr1d0YuLFrTXTsYh4NH3r/YLOcWHEWQM4VWFn9d4T9fY+s2bNori4mLlz554RTgB8fHy47777aNGixS+eo7KykieffJK2bdvi7+9Pq1at+Otf/0pZWdlZj//mm2/o0aMHAQEBJCUl8fHHH9fYn5uby0MPPUTXrl0JCQkhLCyMESNG8OOPP17YxUqtHck/xXWvrmHN/pME+3nz5oQ+3DawjcKJiHg8BZRfYLFYGNa5qrPsNzuy6u19Fi9eTLt27UhOTj7vc9x22208/vjj9OrVi+eff55BgwYxY8YMxo0bd8axe/fuZezYsYwYMYIZM2bg4+PDddddx9KlS53HHDhwgE8++YRRo0bx3HPPMWXKFLZu3cqgQYM4evToedcptbPtSAFXz/6ePdlFRIf68+FdKVzeUevpiEjjoFs8v2JYUgzz1lStnml3GHU+ZbjNZuPo0aOMHj36jH35+flUVlY6nwcHBxMYGHjGcT/++CPz58/ntttu44033gDgT3/6E9HR0fzjH/9g+fLlXHbZZc7j9+zZw7///W+uueYaACZOnEjHjh15+OGHGTp0KABdu3Zlz549NW4r3XjjjXTs2JE5c+bw2GOP1cn1yy9bvjuHSe9uoqTcTmJMKG/d0pfm4Wf+/xcR8VRqQfkVfVtHYA30Jbe4nPXpuXV+fpvNBkBIyJlTOV966aVERUU5H7Nnzz7rOb788ksAJk+eXGP7n//8ZwC++OKLGtvj4uK4+uqrnc/DwsK46aab+OGHH8jKqmop8vf3d4YTu93OyZMnCQkJITExkU2bNp3PpUotLNyYyW3zN1JSbmdAu0gW3p2icCIijY4Cyq/w9fbid6dv83z2Y93f2ggNDQWgqKjojH2vvfYaS5cu5Z133vnVcxw6dAgvLy/atWtXY3tsbCzh4eEcOnSoxvZ27dqd0X+hQ4cOQNW8JgAOh4Pnn3+e9u3b4+/vT9OmTYmKimLLli0UFBTU6hrl3BmGwSsr9jPloy3YHQbX9GzO3Jv7ERagab9FpPFRQPkNv+8RB8BX245RXumo03NbrVaaNWvGtm3bztiXnJzMkCFDGDBgwDmdqy47TT799NNMnjyZSy65hHfeeYevv/6apUuX0rlzZxyOuv0MpIrDYfDUFzt5ZskuAO68pA3P/qG7hhGLSKOlPii/oX+bSKJC/TleWMbqfcfrvJPiyJEjefPNN1m/fj39+vWr9etbtmyJw+Fg7969dOrUybk9Ozub/Pz8M+ZO2bdvH4Zh1Ag0e/bsAapmmgX46KOPuOyyy5gzZ06N1+bn59O0qVbIrWvllQ6mfPQjn26uaqV7dGQnbhvYxuSqRETMpV/PfoO3l4WRXauG/362ue5v8/zlL38hKCiIW2+9lezs7DP2/9Zi01dccQUAL7zwQo3tzz33HFAVgH7q6NGjLFq0yPncZrPx9ttv06NHD2Jjq25neXt7n/G+Cxcu5MiRI+d2UXLOTpXbueP/NvLp5qP4eFl4fmx3hRMREdSCck5+3yOOeWsO8s2ObE6V2wn0q7tJ29q3b8+CBQu4/vrrSUxMdM4kaxgG6enpLFiwAC8vL+Ljzz6bbffu3ZkwYQKvv/46+fn5DBo0iPXr1zN//nxGjx5dYwQPVPU3mThxIhs2bCAmJoa33nqL7Oxs5s6d6zxm1KhRPPHEE9xyyy1cdNFFbN26lXfffZc2bfTFWZdspRVMnLeBDQfzCPD14tUbenNpYrTZZYmIuAQFlHPQs0U4LSICycw9xbc7s7mye1ydnv+qq65i69atPPvss3zzzTe89dZbWCwWWrZsyciRI7nrrrvo3r37L77+zTffpE2bNsybN49FixYRGxvL1KlTmTZt2hnHtm/fnpdeeokpU6awe/duWrduzQcffMDw4cOdx/z1r3+luLiYBQsW8MEHH9CrVy+++OILHnnkkTq97sbsZFEZN721nu1HbYQG+DD35r70aRVhdlkiIi7DYvzWPQQXZLPZsFqtFBQUEBbWMAul/f3rXcxevp+hSTG8cVOfBnlP8UzHCk5xw5vr2H+8mMhgP96e2I/OcVazyxIRcSnqg3KOft+9ajn7lbuPU3CqwuRqxF0dOlnMta+ksv94MXHWAD68K0XhRETkLBRQzlFibCiJMaGU2x18va3+pr4Xz7Uvp4g/vJbKkfxTtG4azMK7L6Jt1JmT9ImIiAJKrVTPiVIfk7aJZ9t5zMbY11LJtpXRISaED+7sr9lhRUR+hQJKLVzZrSqgrNl/gpzCUpOrEXex5XA+17+xlpPF5XSOC+P9O1KIDg0wuywREZemgFILCZFB9GgRjsOAL7YcM7sccQNph3IZ/8Y68ksq6NEinAW39yci2M/sskREXJ4CSi1ddfo2z8ebzJ+07ODBg1gsFubNm3der7dYLEyfPr1Oa5L/Wp+ey01z1lNYVkm/1hG8c1sy1kCtqyMici4UUGrpqh7N8fW2sPVIAduPauE8ObvU/SeZ8NZ6ik+vSDzvlr6E+GvaIRGRc6WfmLUUEezHsM6xfLHlGB9uyOR/rzJviGjLli05deoUvr7n91v5qVOn8PHRX4G69v2+E0ycv4HSCgcD2zfljZv6EOBbd7MPi4g0BrVuQVm1ahVXXnklcXFxWCwWPvnkkxr7DcPg8ccfp1mzZgQGBjJkyBD27t1b45jc3FzGjx9PWFgY4eHhTJw4kaKiogu6kIY0rm8LABb9cITSCrtpdVgsFgICAvD2Pr8vv4CAAAWUOrZqz3FunVcVTi5LjFI4ERE5T7UOKMXFxXTv3p3Zs2efdf+sWbN48cUXefXVV1m3bh3BwcEMHz6c0tL/jnoZP34827dvZ+nSpSxevJhVq1Zxxx13nP9VNLABbZvSPDwQW2klX227sM6y06dPx2KxsGfPHm644QasVitRUVE89thjGIZBZmYmV111FWFhYcTGxvLss886X3u2Pig333wzISEhHDlyhNGjRxMSEkJUVBQPPfQQdnvNMPXzPigXUgvAvHnzsFgsHDx4sMb2FStWYLFYWLFihXPbpZdeSpcuXdiyZQuDBg0iKCiIdu3a8dFHHwGwcuVKkpOTCQwMJDExkW+//faCPueGsGJ3Dre9vZGySgdDOkXz6o29FU5ERM5TrQPKiBEjeOqpp7j66qvP2GcYBi+88AKPPvooV111Fd26dePtt9/m6NGjzpaWnTt3smTJEt58802Sk5O5+OKLeemll3j//fc5etQ95hfx8rIw9nQryvvrM+vknGPHjsXhcDBz5kySk5N56qmneOGFFxg6dCjNmzfnmWeeoV27djz00EOsWrXqV89lt9sZPnw4kZGR/OMf/2DQoEE8++yzvP766w1ey6/Jy8tj1KhRJCcnM2vWLPz9/Rk3bhwffPAB48aN44orrmDmzJkUFxdz7bXXUlhYeN7vVd9W7M7hjv9Lo7zSwbCkGF4e3xt/H4UTEZHzZlwAwFi0aJHz+f79+w3A+OGHH2ocd8kllxj33XefYRiGMWfOHCM8PLzG/oqKCsPb29v4+OOPz/o+paWlRkFBgfORmZlpAEZBQcGFlH9BjuaXGK0fWWy0fHixsT+n8LzPM23aNAMw7rjjDue2yspKIz4+3rBYLMbMmTOd2/Py8ozAwEBjwoQJhmEYRnp6ugEYc+fOdR4zYcIEAzCeeOKJGu/Ts2dPo3fv3jW2Aca0adPqpBbDMIy5c+cagJGenl7jfZYvX24AxvLly53bBg0aZADGggULnNt27dplAIaXl5exdu1a5/avv/76jOt0Jct3ZRvt/+dLo+XDi4073t5glFXYzS5JRMTt1ekonqysqingY2JiamyPiYlx7svKyiI6uuaS8j4+PkRERDiP+bkZM2ZgtVqdjxYtWtRl2eelmTWQSxOrruODjRfeinLbbbc5/+zt7U2fPn0wDIOJEyc6t4eHh5OYmMiBAwd+83x33XVXjecDBw48p9fVRy2/JCQkhHHjxjmfJyYmEh4eTqdOnUhOTnZur/7zhbxXfflpy8nwzjH864+98PPR4DgRkQvlFj9Jp06dSkFBgfORmVk3t1UuVPVtnn+nHabC7rigcyUkJNR4brVaCQgIoGnTpmdsz8vL+9VzBQQEEBUVVWNbkyZNfvN19VHLr4mPj8disZxxzp8HUKu1aqTUhbxXffhpOPld51j+9cde+Hq7xT8pERGXV6c/TWNjYwHIzs6usT07O9u5LzY2lpycnBr7Kysryc3NdR7zc/7+/oSFhdV4uILLO0bTNMSfE0XlLNuZ89sv+BVnG4nzS6NzDMOo9bkaopafh41qP++c+1vnPN/rbkg/Dycv/bGnwomISB2q05+orVu3JjY2lmXLljm32Ww21q1bR0pKCgApKSnk5+eTlpbmPOa7777D4XDUaNZ3B77eXlzbOx6A9zdkmFyN+Zo0aQJAfn5+je2HDh0yoZr6s2rPcYUTEZF6VuufqkVFRWzevJnNmzcDkJ6ezubNm8nIyMBisfDAAw/w1FNP8dlnn7F161Zuuukm4uLiGD16NACdOnXid7/7Hbfffjvr16/n+++/55577mHcuHHExcXV5bU1iOrbPCv3HOdwXonJ1Zirbdu2ADVG9tjt9nMePeQO/rP3OLe/vdHZ50ThRESkftR6lq6NGzdy2WWXOZ9PnjwZgAkTJjBv3jz+8pe/UFxczB133EF+fj4XX3wxS5YsISDgv6u3vvvuu9xzzz0MHjwYLy8vxowZw4svvlgHl9PwWjcN5qK2kazZf5L/Sz3E1Cs6mV2SaTp37kz//v2ZOnUqubm5RERE8P7771NZWWl2aXVi9d4T3Da/ap6ToUkxvHS9+pyIiNSXWgeUSy+99Ff7AlgsFp544gmeeOKJXzwmIiKCBQsW1PatXdbEi1uzZv9JFqzP4L7B7QluxGuuvPvuu9x5553MnDnTOUvwZZddxtChQ80u7YKs2XeC297e4JyEbbZG64iI1CuL4Uo9D8+RzWbDarVSUFDgEh1mHQ6DIc+t5MCJYqZfmcTNA1qbXZLUodT9J7ll3npKKxxc3jGaV27opUnYRETqmX4FrANeXhZuubgqlLz1/UHsDrfLfPIL1h44WWNtHYUTEZGGoYBSR8b0ao410JeM3BK+3Zn92y8Ql7c+PZdb5m7gVIWdQR2ieOUGTV8vItJQFFDqSJCfD+OTqyY4m/OfdJOrkQu18WAuN89dz6kKOwPbN+U1LfwnItKgFFDq0E0prfDxsrD+YC5bDuebXY6cp7RDuUx4az0l5XYubteUN27qo3AiItLAFFDqUKw1gCu7V83lMme1WlHcUdqhPCa8tYHicjsXtY1UOBERMYkCSh2beLqz7BdbjnGs4JTJ1UhtbMrIY8Jb6ykqq+SitpHMmdCXQD+FExERMyig1LEuza0kt46g0mEwb81Bs8uRc/RDRh4T5lSFk5Q2CiciImZTQKkHtw9sA8A7qYfILS43uRr5LT9k5HHTnPUUllXSv00Ec27uo3AiImIyBZR6MLhTNJ3jwigut/P6qgNmlyO/YtNPwkm/1hG8dXNfgvwa70zAIiKuQgGlHlgsFiYP7QDA/DUHOVFUZnJFcjY/DyfzblE4ERFxFQoo9eTyjtF0j7dyqsLOayv3m12O/EzaoapwUlRWSbLCiYiIy1FAqScWi4UHT7eivJ16iBxbqckVSbWqocRV4aR/mwjmKpyIiLgcBZR6NKhDFD0TwimrdPCKWlFcwvr0XG6as84ZTtTnRETENSmg1KOf9kV5d10G2WpFMdWa/SeY8NZ65yRsCiciIq5LAaWeXdyuKX1bNaG80sHLy/eZXU6j9Z+9x50L/w1s31ThRETExSmg1LOf9kV5b30mh/NKTK6o8Vm+O4eJ8zdSVung8o7Rmr5eRMQNKKA0gIvaNuWitpGU2x08/eVOs8tpVL7ensWdb6dRXulgWFIMr96gVYlFRNyBAkoDeWxUEl4W+HJrFmv2nTC7nEbhkx+O8Kd3N1FudzCyazNmj++Fn4/+youIuAP9tG4gnZqFcWP/lgBM/3w7FXaHyRV5tvfWZ/Dgh5uxOwzG9Irnn+N64Outv+4iIu5CP7Eb0INDO9AkyJc92UW8s/aQ2eV4rDmr05n68VYMA27s35K/X9sNH4UTERG3op/aDSg8yI+HhicC8NzSPZoCv44ZhsE/v93Lk4t3AHDnJW144qrOeHlZTK5MRERqSwGlgY3rm0DnuDAKSyv5x9e7zS7HYzgcBv/7+Q6e/3YPAA8O6cAjIzpisSiciIi4IwWUBubtZeF/f98ZgA82ZrLlcL65BXmACruDBz/czLw1BwGYfmUS9w9pr3AiIuLGFFBM0KdVBKN7xGEY8NdFW9Vh9gKcKrdz+9sb+XTzUXy8LLwwtgc3D2htdlkiInKBFFBM8tcrOmEN9GXbERsvfacZZs9HXnE5N8xZx4rdxwnw9eKNm/owumdzs8sSEZE6oIBikuiwAJ4a3QWA2cv3sTkz39yC3ExmbgljXl1D2qE8wgJ8eGdiMpd1jDa7LBERqSMKKCa6snscv+8eh91hMPmDzZwqt5tdklvYcjifq1/+ngPHi4mzBrDwrovo0yrC7LJERKQOKaCY7ImrOhMT5s+BE8XM/ErT4P+WZTuzGfvaWk4UldOpWRiLJg0gMTbU7LJERKSOKaCYLDzIj79f2x2A+amH+M/e4yZX5Lr+b+0hbn97o3NF4g/v7E9MWIDZZYmISD1QQHEBl3SI4qaUqmnwpyzcQl5xuckVuZYKu4PHPtnGY59sw2HAdb3jeevmvoQG+JpdmoiI1BMFFBfxyIiOtGkaTJatlLvfTdPQ49PyS8qZ8NZ6/m/tISwW+MvvEpl1bTetqyMi4uH0U95FBPn58OqNvQn282btgVz+9/PtZpdkun05hVw1+3vW7D9JkJ83r93Qmz9d2k4TsImINAIKKC6kQ0wo/xzXE4sF3lmbwf+lHjS7JNMs2ZbF1bPXcOhkCc3DA/n33RcxrHOs2WWJiEgDUUBxMUOSYphyekHB6Z/vYM2+EyZX1LAq7Q6e/nInd72TRmFZJf1aRfDZPQPo1CzM7NJERKQBKaC4oLsHtWV0j6r5Uf60YBOHThabXVKDyLGV8sc31/H6qgMA3HZxa969PZnIEH+TKxMRkYamgOKCLBYLM8d0o3uLcPJLKrhxznqO5J8yu6x6tWb/Ca54cTXr03MJ8ffhlfG9eHRUkjrDiog0Uvrp76ICfL1548betIwMIiO3hLGvpZKZW2J2WXWurNLO377Ywfg313GiqIyOsaF8ds8ARnRtZnZpIiJiIgUUFxYdFsD7d/SnVWQQh/NOMe71tR4VUnZl2bjqX9/zxn/SMQy4vl8Ci/40gDZRIWaXJiIiJrMYhmGYXURt2Ww2rFYrBQUFhIV5fufJrIJS/vjGWg6cqFp75r07+tMyMtjsss6b3WEw9/t0Zi3ZTbndQWSwH8+M6caQpBizSxMRERehgOImcmylXP/GWvYfLyY2LIA3J/ShS3Or2WXV2rYjBfx10Va2HC4AYHDHaGaO6UZUqDrCiojIfymguJGcwlLGv7GOvTlF+Pl48dToLvyhTwuzyzonRWWVPPvNbuavOYjDgNAAH/56RSfG9W2hiddEROQMCihupqCkgskfbmbZrhwAru/XgmlXdibA19vkys7O4TBYvPUYT3+xkyxbKQBXdo/jsVGdiA7VQn8iInJ2CihuyOEwmL18H899uwfDgK7Nrbw8vhctIoLMLs3JMAxW7T3BrCW72H7UBkBCRBBPju7CoA5RJlcnIiKuTgHFja3ac5z73/+BvJIK/H28uGtQW+4a1JZAP3NbU37IyGPWkt2kHjgJQIi/D7cPbMOdg9q4bEuPiIi4FgUUN3c4r4SHFv7I2gO5ADQPD+SvV3Tiiq6xDdq3o9LuYOmObOZ+f5D1B6tq8fP24ob+LZl0WVvNBisiIrWigOIBDMPgq21Z/O2Lnc4ZZ/u1iuDWi1szuFN0vc7GerKojH9vOsz8NYec7+3jZeGqHs15cGh74pu4zm0nERFxHwooHuRUuZ3XVu3nlRX7Kat0ABAd6s8f+rRgbN8WddZHJdtWytfbs/hqaxbr0k/iOP03qEmQL+OTW3JD/5bEWtUBVkREzp8Cigc6mn+Kt1MP8VFaJieKygGwWKo60/Zu2YQ+LSPo06oJMWG/HSIcDoMDJ4rZeiSfLYcL+CEjnx8P5/PTvzVdm1u5oX8CV/Vorj4mIiJSJxRQPFh5pYNvd2azYF0Gq/edOGN/dKg/kSH+NAnypUmQH9YgX8orHeSXVJBfUk7+qQqO5Z+iuNx+xmt7JoQzokssv+vcjIRI3cYREZG6pYDSSBwrOMX69FzSDuWx8WAeu7JszlszvyXA14sucVa6xlvpFm+lf5tImlkD67dgERFp1BRQGqnC0goOHC8mr6S8RouJn48XTYL8CA/0xRrkS1SIP62bBuNTjx1tRUREfs7H7ALEHKEBvnRvEW52GSIiImelX4tFRETE5SigiIiIiMtRQBERERGXo4AiIiIiLsfUgDJ79mxatWpFQEAAycnJrF+/3sxyRERExEWYFlA++OADJk+ezLRp09i0aRPdu3dn+PDh5OTkmFWSiIiIuAjT5kFJTk6mb9++/Otf/wLA4XDQokUL7r33Xh555JFffa3mQREREfFsprSglJeXk5aWxpAhQ/5biJcXQ4YMITU19Yzjy8rKsNlsNR4iIiLiuUwJKCdOnMButxMTE1Nje0xMDFlZWWccP2PGDKxWq/PRokWLhipVRERETOAWo3imTp1KQUGB85GZmWl2SSIiIlKPTJnqvmnTpnh7e5OdnV1je3Z2NrGxsWcc7+/vj7+/f0OVJyIiIiYzpQXFz8+P3r17s2zZMuc2h8PBsmXLSElJMaMkERERcSGmLRY4efJkJkyYQJ8+fejXrx8vvPACxcXF3HLLLWaVJCIiIi7CtIAyduxYjh8/zuOPP05WVhY9evRgyZIlZ3ScPZvqkdEazSMiIuJ+QkNDsVgsv3qMafOgXIjDhw9rJI+IiIibOpd5zNwyoDgcDo4ePXpOCay2bDYbLVq0IDMzU5PA1SN9zg1Dn3PD0OfcMPQ5N5z6/qzP5fvbtFs8F8LLy4v4+Ph6fY+wsDD9A2gA+pwbhj7nhqHPuWHoc244Zn7WbjEPioiIiDQuCigiIiLichRQfsbf359p06ZpYrh6ps+5Yehzbhj6nBuGPueG4wqftVt2khURERHPphYUERERcTkKKCIiIuJyFFBERETE5SigiIiIiMtRQPmJ2bNn06pVKwICAkhOTmb9+vVml+RRZsyYQd++fQkNDSU6OprRo0eze/dus8vyeDNnzsRisfDAAw+YXYpHOnLkCDfccAORkZEEBgbStWtXNm7caHZZHsVut/PYY4/RunVrAgMDadu2LU8++SQa43FhVq1axZVXXklcXBwWi4VPPvmkxn7DMHj88cdp1qwZgYGBDBkyhL179zZYfQoop33wwQdMnjyZadOmsWnTJrp3787w4cPJyckxuzSPsXLlSiZNmsTatWtZunQpFRUVDBs2jOLiYrNL81gbNmzgtddeo1u3bmaX4pHy8vIYMGAAvr6+fPXVV+zYsYNnn32WJk2amF2aR3nmmWd45ZVX+Ne//sXOnTt55plnmDVrFi+99JLZpbm14uJiunfvzuzZs8+6f9asWbz44ou8+uqrrFu3juDgYIYPH05paWnDFGiIYRiG0a9fP2PSpEnO53a73YiLizNmzJhhYlWeLScnxwCMlStXml2KRyosLDTat29vLF261Bg0aJBx//33m12Sx3n44YeNiy++2OwyPN7IkSONW2+9tca2a665xhg/frxJFXkewFi0aJHzucPhMGJjY42///3vzm35+fmGv7+/8d577zVITWpBAcrLy0lLS2PIkCHObV5eXgwZMoTU1FQTK/NsBQUFAERERJhciWeaNGkSI0eOrPH3WurWZ599Rp8+fbjuuuuIjo6mZ8+evPHGG2aX5XEuuugili1bxp49ewD48ccfWb16NSNGjDC5Ms+Vnp5OVlZWjZ8fVquV5OTkBvtedMvFAuvaiRMnsNvtxMTE1NgeExPDrl27TKrKszkcDh544AEGDBhAly5dzC7H47z//vts2rSJDRs2mF2KRztw4ACvvPIKkydP5q9//SsbNmzgvvvuw8/PjwkTJphdnsd45JFHsNlsdOzYEW9vb+x2O3/7298YP3682aV5rKysLICzfi9W76tvCihiikmTJrFt2zZWr15tdikeJzMzk/vvv5+lS5cSEBBgdjkezeFw0KdPH55++mkAevbsybZt23j11VcVUOrQhx9+yLvvvsuCBQvo3Lkzmzdv5oEHHiAuLk6fswfTLR6gadOmeHt7k52dXWN7dnY2sbGxJlXlue655x4WL17M8uXLiY+PN7scj5OWlkZOTg69evXCx8cHHx8fVq5cyYsvvoiPjw92u93sEj1Gs2bNSEpKqrGtU6dOZGRkmFSRZ5oyZQqPPPII48aNo2vXrtx44408+OCDzJgxw+zSPFb1d5+Z34sKKICfnx+9e/dm2bJlzm0Oh4Nly5aRkpJiYmWexTAM7rnnHhYtWsR3331H69atzS7JIw0ePJitW7eyefNm56NPnz6MHz+ezZs34+3tbXaJHmPAgAFnDJXfs2cPLVu2NKkiz1RSUoKXV82vK29vbxwOh0kVeb7WrVsTGxtb43vRZrOxbt26Bvte1C2e0yZPnsyECRPo06cP/fr144UXXqC4uJhbbrnF7NI8xqRJk1iwYAGffvopoaGhzvuYVquVwMBAk6vzHKGhoWf06wkODiYyMlL9ferYgw8+yEUXXcTTTz/NH/7wB9avX8/rr7/O66+/bnZpHuXKK6/kb3/7GwkJCXTu3JkffviB5557jltvvdXs0txaUVER+/btcz5PT09n8+bNREREkJCQwAMPPMBTTz1F+/btad26NY899hhxcXGMHj26YQpskLFCbuKll14yEhISDD8/P6Nfv37G2rVrzS7JowBnfcydO9fs0jyehhnXn88//9zo0qWL4e/vb3Ts2NF4/fXXzS7J49hsNuP+++83EhISjICAAKNNmzbG//zP/xhlZWVml+bWli9fftafyRMmTDAMo2qo8WOPPWbExMQY/v7+xuDBg43du3c3WH0Ww9BUfCIiIuJa1AdFREREXI4CioiIiLgcBRQRERFxOQooIiIi4nIUUERERMTlKKCIiIiIy1FAEREREZejgCIiIiIuRwFFREREXI4CioiIiLgcBRQRERFxOQooIiIi4nL+H2XjYBewXGr4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3087ae2",
   "metadata": {
    "id": "e3087ae2"
   },
   "source": [
    "Second, because the *loss* is defined over **the entire training set**: that's **expensive** to compute.\n",
    "\n",
    "Even if we calculate the loss over **one example** at a time:\n",
    "\n",
    "$$f = model, \\theta = parameters, x = sample, y = label$$\n",
    "\n",
    "$$\n",
    "\\bbox[5px,border:2px solid red]\n",
    "{\n",
    "Loss(f(x;\\theta),y)\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a06db9e",
   "metadata": {
    "id": "3a06db9e"
   },
   "source": [
    "We want to model not just one sample but a **whole dataset** of size $m$, the loss being an **average** over all samples $i$:\n",
    "\n",
    "\n",
    "$$\n",
    "\\bbox[5px,border:2px solid red]\n",
    "{\n",
    "\\DeclareMathOperator*{\\argmax}{argmax} % thin space, limits underneath in displays\n",
    "J(\\theta) = \\frac{1}{m}\\sum_{i=1}^m Loss(f(x^i;\\theta),y^i)\n",
    "}\n",
    "$$\n",
    "\n",
    "We want to find parameters ($\\theta$) that make this as small as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c1236c-69b0-4400-82d6-fb8ea8aec74f",
   "metadata": {
    "id": "ccc305e2",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### The Rubik problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040b6b05-a5bb-46c7-9f21-3665c93a0135",
   "metadata": {
    "id": "ccc305e2"
   },
   "source": [
    "Every time we calculate the gradient with respect to **one sample**, we are solving for just one side of the Rubik's.\n",
    "\n",
    "Ideally, you'd want to make an update that improves your model for **all of your data** in one go.\n",
    "\n",
    "In the *space of the loss function*, this is equivalent to have 'perfect' gradient: **know exactly** in which direction to step.\n",
    "\n",
    "But there's way too much data! \n",
    "\n",
    "*Intuitively, perhaps you can think about this problem a bit like a Rubik's cube: you don't know which tweak is best to reach the solutions, so you tweak your machine to recognise 1s, which is a bit like solving for one face of the cube, but maybe as you do so, you realise that it now recognises 3s less. So you go on rectifying that, but then you lose on 7s, etc.*\n",
    "\n",
    "We could call this the **Rubik problem**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9e30d2-ab37-4ac1-ba0c-0fc4a9599f12",
   "metadata": {
    "id": "ff9e30d2-ab37-4ac1-ba0c-0fc4a9599f12"
   },
   "source": [
    "This is where gradient descent kicks in, solving our two problems:\n",
    "\n",
    "1. We can improve our loss **without fully understanding its behaviour** (*landscape*).\n",
    "\n",
    "2. The second is **stochastic**/**mini-batch** gradient descent, allowing us to **approximate  the gradient** using just a small batch of our data (one 'rubik face')."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fd890d-1cbf-4a92-b94f-8d39734a8890",
   "metadata": {
    "id": "7128869c-1f10-482d-a37d-a85fef5ef5d7",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### The landscape problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858556db-9d16-4508-81ae-a2dc4dfc9d1f",
   "metadata": {
    "id": "7128869c-1f10-482d-a37d-a85fef5ef5d7"
   },
   "source": [
    "As we have seen, the **loss** is a multivariate function taking **all the parameters** of the network as input.\n",
    "\n",
    "The input space has **as many dimensions as the number of parameters (weights and biases)**.\n",
    "\n",
    "Given the difficulty of the task, we take an **iterative** approach.\n",
    "\n",
    "We start anywhere on that landscape, we improve things a tiny bit, and repeat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c253e707-7a2f-4b53-8c7f-e80f7addfd66",
   "metadata": {
    "id": "c253e707-7a2f-4b53-8c7f-e80f7addfd66"
   },
   "source": [
    "If the input space of the loss is like a **multidimensional landscape**,\n",
    "\n",
    "and each network parameter is a **dimension**,\n",
    "\n",
    "the loss is our **ground height**,\n",
    "\n",
    "the negative of the gradient gives us the **direction of steepest descent** (our *compas*)\n",
    "\n",
    "and the update to our parameters is one step **downhill** to find a minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2e6fc0-b71a-4cfb-804d-67ae7b519851",
   "metadata": {
    "id": "4b2e6fc0-b71a-4cfb-804d-67ae7b519851"
   },
   "source": [
    "<!-- <img style=\"height: 700px;\" src=\"images/loss.landscape.jpeg\"> -->\n",
    "<img style=\"height: 700px;\" src=\"https://raw.githubusercontent.com/jchwenger/DLWP/main/lectures/02/images/loss.landscape.jpeg\">\n",
    "\n",
    "<small>Image Source: A. Amini et al. â€œ[Spatial Uncertainty Sampling for End-to-End Control](https://arxiv.org/abs/1805.04829)â€. *NeurIPS Bayesian Deep Learning 2018*</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5177bfe0-e869-405e-a99f-7ed17b350f1e",
   "metadata": {
    "id": "2bea1900-8972-448b-bc24-7467c93d2320"
   },
   "source": [
    "### Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a1e24b-0bd2-4019-a1b2-d2aa0ce5bcd6",
   "metadata": {
    "id": "2bea1900-8972-448b-bc24-7467c93d2320"
   },
   "source": [
    "$$\n",
    "\\bbox[5px,border:2px solid red]\n",
    "{\n",
    "    \\begin{align*}\n",
    "    w &= w - \\eta \\nabla_w f \\\\\n",
    "    b &= b - \\eta \\nabla_b f\n",
    "    \\end{align*}\n",
    "}\n",
    "$$\n",
    "\n",
    "In Python:\n",
    "\n",
    "$$\n",
    "\\bbox[5px,border:2px solid red]\n",
    "{\n",
    "    \\begin{align*}\n",
    "    w\\ &\\mathrel{{-}{=}} eta * w\\_gradient \\\\\n",
    "    b\\ &\\mathrel{{-}{=}} eta * b\\_gradient\n",
    "    \\end{align*}\n",
    "}\n",
    "$$\n",
    "\n",
    "$\\bf{\\eta}$ **= step size =  learning rate**.\n",
    "\n",
    "By convention, we make $w$ and $\\nabla_w f$ the same shape (big tensors), so that the updates are applied *elementwise*.\n",
    "\n",
    "<small>For reference, here's the official TensorFlow [Introduction to gradients and automatic differentiation](https://www.tensorflow.org/guide/autodiff).</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf039045-88fa-4bde-b192-a424b0ccce09",
   "metadata": {
    "id": "bf039045-88fa-4bde-b192-a424b0ccce09"
   },
   "source": [
    "**If the step size is too small**, the ball might get stuck in a local minimum â€“ never climbing the barrier between the minima. Also, it is **slow**.\n",
    "\n",
    "**But if the step size is too large**, the ball might never find any minimum, it would just keep bouncing around, and could even diverge.\n",
    "\n",
    "ðŸ’€  There is **no way of knowing** the optimal step size because we never know what the loss landscape looks like. Must be tweaked!\n",
    "\n",
    "<!-- <img style=\"height:300px;float:right;\" src=\"images/imperial.learning_rate.svg\"> -->\n",
    "<img style=\"height:300px;float:right;\" src=\"https://raw.githubusercontent.com/jchwenger/DLWP/d942b064df6d0e929cbd027bc6a6939f9e8fc763/2-building-blocks/images/imperial.learning_rate.svg\">\n",
    "\n",
    "<small style=\"position:absolute;bottom:0;right:0;\">Source: [Imperial College Machine Learning - Neural Networks](https://www.doc.ic.ac.uk/~nuric/teaching/imperial-college-machine-learning-neural-networks.html)</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2614cf-10d6-459a-b7a3-5afa47b4caae",
   "metadata": {
    "id": "8a2614cf-10d6-459a-b7a3-5afa47b4caae"
   },
   "source": [
    "Research around optimizers is active, and focusses on solving these problems:\n",
    "\n",
    "- How to learn **as fast as possible** (in particular, faster than SGD).\n",
    "\n",
    "- How to avoid getting stuck in a **local optimum** (slightly less relevant for large nets.).\n",
    "\n",
    "- How not to **overshoot** (you are near your goal and you go past it).\n",
    "\n",
    "- How to move at different speeds in **different dimensions** (automatically).\n",
    "\n",
    "- How to **minimize memory consumption**.\n",
    "\n",
    "A key idea to improve vanilla SGD is **momentum**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685c96c5-073a-4247-8afe-b741c22da969",
   "metadata": {
    "id": "4ff90815-c55d-4684-8d49-4ba59e0aacc9"
   },
   "source": [
    "### Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3345b8d0-859c-41c7-82e7-bb68afbb3e60",
   "metadata": {
    "id": "4ff90815-c55d-4684-8d49-4ba59e0aacc9"
   },
   "source": [
    "Momentum can be understood in two ways:\n",
    "\n",
    "- from **physics**, with the ball on slope analogy: an acceleration is added to the velocity and the velocity is added to position.  \n",
    "- as an **exponential moving average** (EMA) (where the previous gradients still have an influence on the current one)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3993b5a4-b515-474f-acb4-4b456ed17923",
   "metadata": {
    "id": "3993b5a4-b515-474f-acb4-4b456ed17923"
   },
   "source": [
    "<!-- <img style=\"height:500px\" src=\"images/khandewal.momentum.png\"> -->\n",
    "<img style=\"height:500px\" src=\"https://raw.githubusercontent.com/jchwenger/DLWP/main/lectures/02/images/khandewal.momentum.png\">\n",
    "\n",
    "<small>[Harsh Khandewal, *Gradient Descent with Momentum, RMSprop And Adam Optimizer*, Medium](https://medium.com/analytics-vidhya/momentum-rmsprop-and-adam-optimizer-5769721b4b19)</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328a5676-ebba-4dea-9284-42d67d585275",
   "metadata": {
    "id": "328a5676-ebba-4dea-9284-42d67d585275"
   },
   "source": [
    "Particle dynamics:\n",
    "    \n",
    "$\\begin{cases}\n",
    "v(t + \\Delta t) = v(t) + a(t) \\Delta t\\\\\n",
    "x(t + \\Delta t) = x(t) + v(t) \\Delta t\n",
    "\\end{cases}$\n",
    "    \n",
    "Or, with $\\Delta t = 1$\n",
    "    \n",
    "$\\begin{cases}\n",
    "v(t + 1) = v(t) + a(t) \\\\\n",
    "x(t + 1) = x(t) + v(t + 1)\n",
    "\\end{cases}$\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de532658-05f8-46a0-8ce3-9b127ffe6731",
   "metadata": {
    "id": "de532658-05f8-46a0-8ce3-9b127ffe6731"
   },
   "source": [
    "The acceleration of a ball has two parts: friction and gradient.\n",
    "\n",
    "$\\color{red}{\\alpha - 1}$: the coefficient of friction, values between 0 and 1.  \n",
    "$\\color{blue}{\\eta } $: step size, aka learning rate.  \n",
    "$\\color{blue}{\\nabla f}$: our gradient (*direction of steepest ascent*).  \n",
    "\n",
    "\\begin{align*}\n",
    "a(t) = \\color{red}{(\\alpha - 1)}v(t) - \\color{blue}{\\eta \\nabla f},\\quad  \\alpha \\in [0, 1]\n",
    "\\end{align*}\n",
    "    \n",
    "\\begin{align*}\n",
    "\\begin{cases}\n",
    "\\color{green}{v(t + 1)} &= v(t) + \\color{red}{(\\alpha - 1)}v(t)  - \\color{blue}{\\eta \\nabla f} \\\\\n",
    "         &= \\color{red}{\\alpha} v(t) - \\color{blue}{\\eta \\nabla f}  & \\leftarrow \\color{red}{(\\alpha - 1)}\\ \\text{simplifies} \\\\\n",
    "x(t + 1) &= x(t) + \\color{green}{v(t + 1)}\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "The negative gradient is the **downwards force** and the **opposing friction** is proportional to velocity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c67fad4-7c26-4a4f-869d-d00c3163208b",
   "metadata": {
    "id": "5c67fad4-7c26-4a4f-869d-d00c3163208b"
   },
   "source": [
    "Instead of this:\n",
    "\n",
    "$$\n",
    "\\bbox[5px,border:2px solid red]\n",
    "{\n",
    "    \\begin{align*}\n",
    "    w &= w - \\eta \\nabla_w f \\\\\n",
    "    \\end{align*}\n",
    "}\n",
    "$$\n",
    "\n",
    "We initialize $v_0$ with zeros, then do:\n",
    "\n",
    "$$\n",
    "\\bbox[5px,border:2px solid red]\n",
    "{\n",
    "    \\begin{cases}\n",
    "        \\begin{align*}\n",
    "            v_t &= \\alpha v_{t-1} - \\eta \\nabla_w f &&| \\leftarrow \\text{updating our velocity}\\\\\n",
    "            w &= w + v_t &&| \\leftarrow \\text{updating our weights} \\\\\n",
    "        \\end{align*}\n",
    "    \\end{cases}\n",
    "}\n",
    "$$\n",
    "\n",
    "$\\bf{\\eta}$ **= step size = learning rate**  \n",
    "$\\bf{\\alpha}$ **= momentum coefficient**\n",
    "\n",
    "\n",
    "\n",
    "<!-- $\\begin{cases}\n",
    "\\Delta w(t + 1) = \\alpha \\Delta w(t) - \\eta \\nabla f  \\\\\n",
    "w(t + 1) = w(t) + \\Delta w(t + 1)\n",
    "\\end{cases}$\n",
    " -->\n",
    "\n",
    "And similarly for $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41253ab9-1111-4522-bbb3-785118ae1ce5",
   "metadata": {
    "id": "41253ab9-1111-4522-bbb3-785118ae1ce5"
   },
   "source": [
    "As pseudocode:\n",
    "\n",
    "```python\n",
    "delta_w = 0\n",
    "loop:\n",
    "    v = alpha * v - eta * gradient\n",
    "    w = w + v\n",
    "```\n",
    "\n",
    "`eta` = `step_size` = `learning_rate`  \n",
    "`alpha` = momentum coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa24aafa-38d4-4cae-b38c-a248ac9fad4c",
   "metadata": {
    "id": "aa24aafa-38d4-4cae-b38c-a248ac9fad4c"
   },
   "source": [
    "Another way of looking at the phenomenon: the current value being an **exponential moving average**.\n",
    "\n",
    "Aka the **smoothing** of your values.\n",
    "\n",
    "\"If your gradient was big for a while, even if current values are low, it take some time to go down.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0168545-f9c0-445b-9ef7-2c61473de4a4",
   "metadata": {
    "id": "d0168545-f9c0-445b-9ef7-2c61473de4a4"
   },
   "source": [
    "<!-- ![Ng exponentially weighted averages](images/andrew-ng-exponentially-weighted-averages.png) -->\n",
    "<img src=\"https://raw.githubusercontent.com/jchwenger/DLWP/main/lectures/02/images/andrew-ng-exponentially-weighted-averages.png\">\n",
    "\n",
    "<small>[Andrew Ng, Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization (Course 2 of the Deep Learning Specialization)](https://www.youtube.com/watch?v=k8fTYJPd3_I&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=20).</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f210bee2-9664-45e1-a33d-57a72abeff8b",
   "metadata": {
    "id": "f210bee2-9664-45e1-a33d-57a72abeff8b"
   },
   "source": [
    "Instead of this:\n",
    "\n",
    "$$\n",
    "\\bbox[5px,border:2px solid red]\n",
    "{\n",
    "    \\begin{align*}\n",
    "    w &= w - \\eta \\nabla_w f \\\\\n",
    "    \\end{align*}\n",
    "}\n",
    "$$\n",
    "  \n",
    "We initialize $v_0$ with zeros, then:\n",
    "\n",
    "$$\n",
    "\\bbox[5px,border:2px solid red]\n",
    "{\n",
    "    \\begin{cases}\n",
    "        \\begin{align*}\n",
    "            v_t &= \\beta v_{t-1} + (1 - \\beta) \\nabla_w f &&| \\leftarrow \\text{updating our EMA*}\\\\\n",
    "            w &= w + \\eta v_t &&| \\leftarrow \\text{updating our weights} \\\\\n",
    "        \\end{align*}\n",
    "    \\end{cases}\n",
    "}\n",
    "$$\n",
    "\n",
    "$\\bf{\\eta}$ **= step size = learning rate**  \n",
    "$\\bf{\\beta}$ **= smoothing coefficient**  \n",
    "\\* EMA: exponential moving average.\n",
    "\n",
    "<!-- $\\begin{cases}\n",
    "\\Delta w(t + 1) = \\alpha \\Delta w(t) - \\eta \\nabla f  \\\\\n",
    "w(t + 1) = w(t) + \\Delta w(t + 1)\n",
    "\\end{cases}$\n",
    " -->\n",
    "  \n",
    "And similarly for $b$.\n",
    "\n",
    "*This is in fact just a scaled version of the previous equations, shifting the value of $\\eta$.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842e68dc-5753-4d6b-aa64-d2f8b9a8513e",
   "metadata": {
    "id": "842e68dc-5753-4d6b-aa64-d2f8b9a8513e",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "This is just scratching the surface on optimizers. Here are the most common:\n",
    "\n",
    "1. **SGD with momentum**\n",
    "(The less costly method, and the first developed historically, can outperform the ones below, but tricky/requires tuning)\n",
    "\n",
    "2. **RMSProp**\n",
    "(used by Chollet all the time, scales the gradient for each dimension)\n",
    "\n",
    "3. **Adam**\n",
    "(frankly, the most popular out there, literally 1 & 2 together!)\n",
    "\n",
    "Great blog posts by Sebastian Ruder [here](https://ruder.io/optimizing-gradient-descent) and [here](https://ruder.io/deep-learning-optimization-2017)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b0246c-dc50-4d33-a3d6-d0e695da97fc",
   "metadata": {
    "id": "4e426844-c38c-475d-8aa5-ad2ca300e3a0"
   },
   "source": [
    "### Recommended learning rate to start with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3ad3e3-796e-4575-9211-570cc54c8239",
   "metadata": {
    "id": "4e426844-c38c-475d-8aa5-ad2ca300e3a0",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "- `1e-4` (Chollet)\n",
    "- #### `3e-4` (many others)\n",
    "\n",
    "(In that ballpark: needs to be tweaked âš™ï¸.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c949e91f-73a6-4b38-acf7-f5eef3e4aa17",
   "metadata": {
    "id": "4e426844-c38c-475d-8aa5-ad2ca300e3a0"
   },
   "source": [
    "### Momentum for SGD: `0.9`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250df781-7b57-43b4-b6c5-e5ece227a9bb",
   "metadata": {
    "id": "4e426844-c38c-475d-8aa5-ad2ca300e3a0",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Keras has all these default settings built-in.\n",
    "\n",
    "You can check the default parameters [in the documentation](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf139b31-d212-4e26-b8b8-5b3259891d29",
   "metadata": {
    "id": "65abc023-a39a-428c-ad4d-d6e608d127df"
   },
   "source": [
    "### Recommended playground"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed6bca0-f4d0-4d89-a152-824288d20f42",
   "metadata": {
    "id": "65abc023-a39a-428c-ad4d-d6e608d127df"
   },
   "source": [
    "[\"Why Momentum Really Works\", by Gabriel Oh, on Distill](https://distill.pub/2017/momentum/), with:\n",
    "1. an interactive graph!\n",
    "2. a lot of math, for those who crave that.\n",
    "\n",
    "<!-- <a href=\"https://distill.pub/2017/momentum/\"><img style=\"height:450px; float:right\" src=\"images/gabriel-oh-momentum.png\"></a> -->\n",
    "<a href=\"https://distill.pub/2017/momentum/\"><img style=\"height:450px; float:right\" src=\"https://raw.githubusercontent.com/jchwenger/DLWP/main/lectures/02/images/gabriel-oh-momentum.png\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d671a74-2969-4fa5-9086-d04fad7aafd0",
   "metadata": {
    "id": "5461f7d9-5518-4725-99a4-e6c18ff1ded0"
   },
   "source": [
    "#### The Rubik's problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e275a9f-4584-46e9-b882-6766f73fe1fc",
   "metadata": {
    "id": "5461f7d9-5518-4725-99a4-e6c18ff1ded0"
   },
   "source": [
    "The loss is the average of the losses for each sample in the entire training set (the **batch**).\n",
    "\n",
    "Batch gradient descent gives us the true gradient but is still prohibitively expensive.\n",
    "\n",
    "We can use the gradient for **one sample**, cheap but *high variance*, and iterate many times: **stochastic gradient descent**.\n",
    "\n",
    "But there is a tradeoff!\n",
    "\n",
    "We can calculate the loss and gradient for small, randomly selected subsets of the training set â€“ **mini-batch (stochastic) gradient descent**.\n",
    "\n",
    "Our gradient approximation is better, and the compute is manageable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0816cc8d-02be-4d21-846f-0c3dc0d0f3a2",
   "metadata": {
    "id": "67259cb0-6ec4-4c23-94f8-6b2e94e45768"
   },
   "source": [
    "$$\n",
    "\t\\qquad \\qquad \\qquad \\underleftrightarrow{\\bf{\\text{batch} \\qquad \\qquad \\qquad \\text{mini-batch} \\qquad \\qquad \\text{stochastic}}}\n",
    "$$\n",
    "$$\n",
    "    \\begin{array}{r|cc}\n",
    "    \\bf{\\text{data}} & \\text{whole dataset} \\qquad & \\text{a few samples}  & \\qquad \\text{one sample}\\\\\n",
    "    \\bf{\\text{compute}} & \\text{intractable} \\qquad  &  \\text{manageable}  &  \\qquad \\text{cheap}\\\\\n",
    "    \\bf{\\text{gradient}} & \\text{exact} \\qquad  &  \\text{intermediate}  &  \\qquad \\text{high variance}\\\\\n",
    "    \\end{array}\n",
    "$$\n",
    "\n",
    "Calculating the gradient on a mini-batch only will yield an **approximation** of the real gradient.\n",
    "\n",
    "Better than a single sample, cheaper than the whole batch!\n",
    "\n",
    "What we lose in precision, we gain in training speed!\n",
    "\n",
    "ðŸ’€ Guess what: **no magic formula** for the ideal batch size. Must be tweaked, depends on hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f40cc81-6c2b-4cc6-ab7d-a759c2ef18ec",
   "metadata": {
    "id": "67259cb0-6ec4-4c23-94f8-6b2e94e45768"
   },
   "source": [
    "##### Rule of thumb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d24da9-b506-4a05-b62f-10879d71b8d2",
   "metadata": {
    "id": "67259cb0-6ec4-4c23-94f8-6b2e94e45768"
   },
   "source": [
    "If your batch size is big, your gradient is more accurate, so you can increase your learning rate! âš™ï¸ Tweak!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ba1aee-da83-4c26-a22e-25034af41255",
   "metadata": {
    "id": "1c5cbdff"
   },
   "source": [
    "## Next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81695e86-1ca7-49ed-9b5a-c133b0351488",
   "metadata": {
    "id": "1c5cbdff"
   },
   "source": [
    "- 1. Derivatives\n",
    "- 2. Derivatives for tensors: the gradient\n",
    "- 3. Optimization: stochastic gradient descent, momentum\n",
    "- 4. **Chaining derivatives: the backpropagation algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209cf033-60a7-4ba7-8dc7-3339e1bc61b4",
   "metadata": {
    "id": "a4d3be55"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec7aa6e-e4cf-4d83-b42b-de60ee73755c",
   "metadata": {
    "id": "a4d3be55"
   },
   "source": [
    "## Chaining derivatives: The Backpropagation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc1492b",
   "metadata": {
    "id": "ecc1492b"
   },
   "source": [
    "The gradient in SGD optimisers is calculated by an efficient organisation of the chain rule for differentiable loss and layer functions: **backpropagation**.\n",
    "\n",
    "It is automatically implemented in TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accdd214",
   "metadata": {
    "id": "accdd214"
   },
   "source": [
    "The revolution of automatic differentiation (TensorFlow, PyTorch, JAX, etc.) is **all based on two principles**:\n",
    "\n",
    "- Once you implement the derivative of an operation (`+`, or `*`, or anything more complex), it is **the same** wherever it appears (duh!, but this wouldn't be useful, if we didn't also have:);\n",
    "\n",
    "- The chain rule means everything is **modular**: we only need to implement **local operations**, then we **chain** them together.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97703954",
   "metadata": {
    "id": "97703954"
   },
   "source": [
    "What does modular mean?\n",
    "\n",
    "That each node only needs:\n",
    "  - to know how to **compute the gradient** of its operation (\"if the op is $x^2$, the gradient will be $2x$\");\n",
    "  - to know the **previous gradient(s)** flowing back from downward in the net;  \n",
    "  - and **the values at the node during the forward pass** (they must be stored during the forward pass).\n",
    "  \n",
    "\n",
    "And that's it! With it you can calculate the gradient coming out, and pass it to the neurons upstream.\n",
    "\n",
    "##### Note\n",
    "\n",
    "If there is more than one branch coming in, the gradients are summed up!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ce014b",
   "metadata": {
    "id": "61ce014b"
   },
   "source": [
    "<!-- ![Stanford backprop](images/stanford-backprop/backprop.13.png) -->\n",
    "<img src=\"https://raw.githubusercontent.com/jchwenger/DLWP/main/lectures/02/images/stanford-backprop/backprop.13.png\">\n",
    "\n",
    "<small>Source: [CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/2017/syllabus.html), lecture 4</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e4e092",
   "metadata": {
    "id": "81e4e092"
   },
   "source": [
    "What have we here: a node. It can be any operation combining the inputs $x$ and $y$ to obtain $z$.\n",
    "\n",
    "This node knows how to compute $\\frac{\\partial z}{\\partial x}$ and $\\frac{\\partial z}{\\partial y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ae6e27",
   "metadata": {
    "id": "40ae6e27"
   },
   "source": [
    "<!-- ![Stanford backprop](images/stanford-backprop/backprop.14.png) -->\n",
    "<img src=\"https://raw.githubusercontent.com/jchwenger/DLWP/main/lectures/02/images/stanford-backprop/backprop.14.png\">\n",
    "\n",
    "Source: [CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/2017/syllabus.html), lecture 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f590985",
   "metadata": {
    "id": "0f590985"
   },
   "source": [
    "During backprop, information flows backward from the loss to the input: the node will receive $\\frac{\\partial L}{\\partial z}$, *the gradient up to this point*, without knowing at all what the operations are or how it was computed. It just receives that number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d8da9e",
   "metadata": {
    "id": "67d8da9e"
   },
   "source": [
    "<!-- ![Stanford backprop](images/stanford-backprop/backprop.15.png) -->\n",
    "<img src=\"https://raw.githubusercontent.com/jchwenger/DLWP/main/lectures/02/images/stanford-backprop/backprop.15.png\">\n",
    "\n",
    "Source: [CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/2017/syllabus.html), lecture 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dc945f",
   "metadata": {
    "id": "f3dc945f"
   },
   "source": [
    "Thanks to the chain rule, the node can then compute $\\frac{\\partial z}{\\partial x}$ and multiply that by $\\frac{\\partial L}{\\partial z}$ to obtain the gradient flowing in the $x$ direction, $\\frac{\\partial L}{\\partial x}$.\n",
    "\n",
    "##### Note\n",
    "\n",
    "In order to compute $\\frac{\\partial z}{\\partial x}$, the node needs to know what $x$ was in the forward pass!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49188e82",
   "metadata": {
    "id": "49188e82"
   },
   "source": [
    "<!-- ![Stanford backprop](images/stanford-backprop/backprop.16.png) -->\n",
    "<img src=\"https://raw.githubusercontent.com/jchwenger/DLWP/main/lectures/02/images/stanford-backprop/backprop.16.png\">\n",
    "\n",
    "Source: [CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/2017/syllabus.html), lecture 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b4773a",
   "metadata": {
    "id": "11b4773a"
   },
   "source": [
    "And the same happens for $\\frac{\\partial L}{\\partial y}$, multiplying $\\frac{\\partial L}{\\partial z}$ by $\\frac{\\partial z}{\\partial y}$, which amounts to *routing* the gradient information upstream!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c445270",
   "metadata": {
    "id": "1c445270"
   },
   "source": [
    "This node has **no idea** how complex the whole network is!\n",
    "\n",
    "It is only one cog in the big machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cb607e",
   "metadata": {
    "id": "93cb607e"
   },
   "source": [
    "The most important things to understand:\n",
    "\n",
    "- A neural net can be represented as a **(directed acyclic) graph** or **DAG** (several paths, but no loops), where information flows from input(s) to output(s), passing by a series of nodes (operations);\n",
    "\n",
    "- When doing backprop, information flows **backward** through the network, computing the **influence** of each node on the loss!\n",
    "\n",
    "- The chain rule says that if *if you know the gradient up to this point*, and the local gradient, you can move back one step by **multiplying** the incoming gradient with the current one;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f56eb21",
   "metadata": {
    "id": "5f56eb21"
   },
   "source": [
    "<!-- ![Stanford backprop](images/stanford-backprop/backprop.01.png) -->\n",
    "<img src=\"https://raw.githubusercontent.com/jchwenger/DLWP/main/lectures/02/images/stanford-backprop/backprop.01.png\">\n",
    "\n",
    "<small>Source: [CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/2017/syllabus.html), lecture 4</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ff7619",
   "metadata": {
    "id": "49ff7619"
   },
   "source": [
    "Here is an example. Our equation is\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(x, y, z) &= (x + y)z\\\\\n",
    "\\text{With:}&\\\\\n",
    "x &= -2\\\\\n",
    "y &= 5\\\\\n",
    "z &= -4\\\\\n",
    "\\text{We have:}\\\\ -2 + 5 &= 3\\\\\n",
    "3 * -4 &= -12\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7498773",
   "metadata": {
    "id": "b7498773"
   },
   "source": [
    "<!-- ![Stanford backprop](images/stanford-backprop/backprop.02.png) -->\n",
    "<img src=\"https://raw.githubusercontent.com/jchwenger/DLWP/main/lectures/02/images/stanford-backprop/backprop.02.png\">\n",
    "\n",
    "<small>Source: [CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/2017/syllabus.html), lecture 4</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f86321",
   "metadata": {
    "id": "57f86321"
   },
   "source": [
    "All the partial derivatives have been calculated for us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3b2b8d",
   "metadata": {
    "id": "5b3b2b8d"
   },
   "source": [
    "<!-- ![Stanford backprop](images/stanford-backprop/backprop.03.png) -->\n",
    "<img src=\"https://raw.githubusercontent.com/jchwenger/DLWP/main/lectures/02/images/stanford-backprop/backprop.03.png\">\n",
    "<small>Source: [CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/2017/syllabus.html), lecture 4</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd747615",
   "metadata": {
    "id": "cd747615"
   },
   "source": [
    "What is $\\frac{\\partial f}{\\partial f}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70336b79",
   "metadata": {
    "id": "70336b79"
   },
   "source": [
    "<!-- ![Stanford backprop](images/stanford-backprop/backprop.04.png) -->\n",
    "<img src=\"https://raw.githubusercontent.com/jchwenger/DLWP/main/lectures/02/images/stanford-backprop/backprop.04.png\">\n",
    "\n",
    "<small>Source: [CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/2017/syllabus.html), lecture 4</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e61355",
   "metadata": {
    "id": "16e61355"
   },
   "source": [
    "One of course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc78aa43",
   "metadata": {
    "id": "fc78aa43"
   },
   "source": [
    "<!-- ![Stanford backprop](images/stanford-backprop/backprop.05.png) -->\n",
    "<img src=\"https://raw.githubusercontent.com/jchwenger/DLWP/main/lectures/02/images/stanford-backprop/backprop.05.png\">\n",
    "\n",
    "<small>Source: [CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/2017/syllabus.html), lecture 4</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986d3302",
   "metadata": {
    "id": "986d3302"
   },
   "source": [
    "Now what is $\\frac{\\partial f}{\\partial z}$? On the left we have the derivative of the op: $\\frac{\\partial f}{\\partial z} = q$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9a5b8e",
   "metadata": {
    "id": "8e9a5b8e"
   },
   "source": [
    "<!-- ![Stanford backprop](images/stanford-backprop/backprop.06.png) -->\n",
    "<img src=\"https://raw.githubusercontent.com/jchwenger/DLWP/main/lectures/02/images/stanford-backprop/backprop.06.png\">\n",
    "\n",
    "<small>Source: [CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/2017/syllabus.html), lecture 4</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805aafa7",
   "metadata": {
    "id": "805aafa7"
   },
   "source": [
    "It is 3, that is $q$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccac9e38",
   "metadata": {
    "id": "ccac9e38"
   },
   "source": [
    "<!-- ![Stanford backprop](images/stanford-backprop/backprop.07.png) -->\n",
    "<img src=\"https://raw.githubusercontent.com/jchwenger/DLWP/main/lectures/02/images/stanford-backprop/backprop.07.png\">\n",
    "\n",
    "<small>Source: [CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/2017/syllabus.html), lecture 4</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010b3916",
   "metadata": {
    "id": "010b3916"
   },
   "source": [
    "Now what is $\\frac{\\partial f}{\\partial q}$? The equation on the left tells us: $\\frac{\\partial f}{\\partial q} = z$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba52ed08",
   "metadata": {
    "id": "ba52ed08"
   },
   "source": [
    "<!-- ![Stanford backprop](images/stanford-backprop/backprop.08.png) -->\n",
    "<img src=\"https://raw.githubusercontent.com/jchwenger/DLWP/main/lectures/02/images/stanford-backprop/backprop.08.png\">\n",
    "\n",
    "<small>Source: [CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/2017/syllabus.html), lecture 4</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bed3cfd",
   "metadata": {
    "id": "7bed3cfd"
   },
   "source": [
    "It's -4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78cb67a",
   "metadata": {
    "id": "f78cb67a"
   },
   "source": [
    "<!-- ![Stanford backprop](images/stanford-backprop/backprop.09.png) -->\n",
    "<img src=\"https://raw.githubusercontent.com/jchwenger/DLWP/main/lectures/02/images/stanford-backprop/backprop.09.png\">\n",
    "\n",
    "<small>Source: [CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/2017/syllabus.html), lecture 4</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281f5b61",
   "metadata": {
    "id": "281f5b61"
   },
   "source": [
    "Now we can move further up. The gradient of additions is just one, so what does that node do? It multiplies the incoming gradient by 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b602e9",
   "metadata": {
    "id": "40b602e9"
   },
   "source": [
    "<!-- ![Stanford backprop](images/stanford-backprop/backprop.10.png) -->\n",
    "<img src=\"https://raw.githubusercontent.com/jchwenger/DLWP/main/lectures/02/images/stanford-backprop/backprop.10.png\">\n",
    "\n",
    "<small>Source: [CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/2017/syllabus.html), lecture 4</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dcdd03",
   "metadata": {
    "id": "11dcdd03"
   },
   "source": [
    "Which means the gradient at $y$ will also be -4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdb2cf4",
   "metadata": {
    "id": "dfdb2cf4"
   },
   "source": [
    "<!-- ![Stanford backprop](images/stanford-backprop/backprop.12.png) -->\n",
    "<img src=\"https://raw.githubusercontent.com/jchwenger/DLWP/main/lectures/02/images/stanford-backprop/backprop.12.png\">\n",
    "\n",
    "<small>Source: [CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/2017/syllabus.html), lecture 4</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d279de4",
   "metadata": {
    "id": "5d279de4"
   },
   "source": [
    "And finally the same for $x$, -4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dc2cd7-38d8-4719-b4d4-bf09553dfce9",
   "metadata": {
    "id": "cb4dc1e1"
   },
   "source": [
    "##### Note\n",
    "\n",
    "Several branches going 'back' into a node (i.e. if above we used `q` for several operations downstream) result in gradients **being summed together** (the mirror image of the summation in the forward pass)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cb1bd9-a5c6-484b-b5d2-b0fd40fa2aab",
   "metadata": {
    "editable": true,
    "id": "cb2563e0",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### A concrete example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3577e0-c3f6-4c1b-971d-a04eb67cd504",
   "metadata": {
    "editable": true,
    "id": "cb2563e0",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The simplest example I could find for the **chain rule**, illustrating the gradient swap for multiplication.\n",
    "\n",
    "$$ ðŸƒ \\leftarrow_{2x} ðŸš² \\leftarrow_{3x} ðŸš— \\quad | \\quad f=3y, y=2x$$\n",
    "\n",
    "\n",
    "How much faster than me does the car go? 2 x 3, right?\n",
    "\n",
    "The gradient for car-to-bike ($ \\frac{df}{dy}  $) is 3, and bike-to-me ($ \\frac{dy}{dx}  $) is  is 2 $\\rightarrow$ the car-to-me ($ \\frac{df}{dy}\\frac{dy}{dx}$)  is 6.\n",
    "\n",
    "What does that mean? It means that if I nudge **bike-to-me** by 1, the car-to-me goes up by 3 (the car-to-bike gradient):\n",
    "- bike-to-me at 3 instead of 2 â†’ car-to-me is now: $3 * 3 = 9\\ (6 + 3)$\n",
    "\n",
    "Conversely, if I nudge the **car-to-bike** by 1, car-to-me will be nudged by 2 (the bike-to-me gradient):\n",
    "\n",
    "- car-to-bike at 4 instead of 3 â†’ car-to-me is now: $2 * 4 = 8\\ (6 + 2)$\n",
    "\n",
    "Here is an [interactive website](https://webspace.ship.edu/msrenault/geogebracalculus/derivative_intuitive_chain_rule.html) allowing you to play with this!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab17638d-4b9e-44f6-a6aa-34bf3214b82b",
   "metadata": {
    "id": "7c56d714"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2842760-5478-424f-9e75-a9cb4de50e12",
   "metadata": {
    "id": "7c56d714"
   },
   "source": [
    "### WE MADE IT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0349ae-6ec3-4b68-9649-69a257652186",
   "metadata": {
    "id": "7c56d714"
   },
   "source": [
    "What does this all mean for us?\n",
    "\n",
    "Automatic differentiation allows us to construct **arbitrary complex graphs of operations**.\n",
    "\n",
    "And we don't need ever to do the mathematical derivation by hand!\n",
    "\n",
    "This may not feel like it now, but this is **hugely** simplifying our lives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd326ee-b5b8-459b-a70d-cd4950287e5b",
   "metadata": {
    "id": "ac5b9260"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cd937a-8ee2-4244-ab4a-876125523728",
   "metadata": {
    "id": "ac5b9260"
   },
   "source": [
    "## 3Blue1Brown summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e1842b-5fa4-4a82-a528-d5c8fd9c3164",
   "metadata": {},
   "source": [
    "[3Blue1Brown |  Gradient descent, how neural networks learn | Deep Learning Chapter 2 ](https://www.youtube.com/watch?v=IHZwWFHWa-w)\n",
    "\n",
    "[3Blue1Brown | Backpropagation, intuitively | Deep Learning Chapter 3](https://www.youtube.com/watch?v=Ilg3gGewQ5U)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c09708c-7d91-4866-a006-3851cb7f97f8",
   "metadata": {
    "id": "8c5536ef-cdaa-4162-a84f-689005995b72"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c0faca-5839-42db-9731-ac55958fa804",
   "metadata": {
    "id": "8c5536ef-cdaa-4162-a84f-689005995b72"
   },
   "source": [
    "# Looking back at our first example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3a72f1-5295-48cf-b858-741239616c46",
   "metadata": {
    "id": "9c3a72f1-5295-48cf-b858-741239616c46"
   },
   "source": [
    "**What we've seen:**\n",
    "\n",
    "- Our first network;\n",
    "\n",
    "- Tensors and their operations recap;\n",
    "\n",
    "- SGD, differentiation and the chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380af092-bec5-4e42-bd66-c1e434a48423",
   "metadata": {
    "id": "380af092-bec5-4e42-bd66-c1e434a48423"
   },
   "source": [
    "All this for **that**??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d45ab7f-dafb-4f2d-be89-413bc6a45e31",
   "metadata": {
    "id": "7d45ab7f-dafb-4f2d-be89-413bc6a45e31"
   },
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6779e3ea-78f8-49db-8656-9114d9cf1800",
   "metadata": {
    "id": "6779e3ea-78f8-49db-8656-9114d9cf1800"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.Input((28 * 28, )))\n",
    "model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cb45396-9ca9-47e4-9c1b-5742383bff30",
   "metadata": {
    "id": "4cb45396-9ca9-47e4-9c1b-5742383bff30"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8007f1b",
   "metadata": {
    "id": "a8007f1b"
   },
   "source": [
    "Before training, our model performs poorly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1b6a06e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e1b6a06e",
    "outputId": "2d9c6840-abac-4f48-8431-371f8f6d0a32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.1303 - loss: 2.3224\n",
      "\n",
      "0.1339000016450882 (*Î¼_Î¼)\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels) # testing before training\n",
    "print()\n",
    "print(test_acc, '(*Î¼_Î¼)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d59649a",
   "metadata": {
    "id": "8d59649a"
   },
   "source": [
    "Then we train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b71f438-1844-4343-9a64-7f89b41f60f8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5b71f438-1844-4343-9a64-7f89b41f60f8",
    "outputId": "ed956b37-aff2-4d53-9fe7-396d445a2b62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8752 - loss: 0.4366\n",
      "Epoch 2/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9647 - loss: 0.1167\n",
      "Epoch 3/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9793 - loss: 0.0719\n",
      "Epoch 4/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9844 - loss: 0.0524\n",
      "Epoch 5/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9892 - loss: 0.0374\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7d2d94209cc0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81999ca-135d-43c6-8d7f-9795936fa16a",
   "metadata": {
    "id": "f81999ca-135d-43c6-8d7f-9795936fa16a"
   },
   "source": [
    "##### Note\n",
    "\n",
    "\n",
    "There are $\\lceil(60000 / 128)\\rceil * 5 = 2345$ gradient updates for 5 epochs of 128 sample mini-batches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01aef2e6",
   "metadata": {
    "id": "01aef2e6"
   },
   "source": [
    "The network is evaluated on the unseen test set. If successful, the trained model can be deployed.\n",
    "\n",
    "In post office sorting rooms, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "869dbbc3-d9c1-4408-bb85-f32a26181bd2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "869dbbc3-d9c1-4408-bb85-f32a26181bd2",
    "outputId": "86accbdd-3cfc-47ca-a9ea-c0d359f02173"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9749 - loss: 0.0813\n",
      "\n",
      "0.9782000184059143 (â•¯âœ§â–½âœ§)â•¯\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print()\n",
    "print(test_acc, '(â•¯âœ§â–½âœ§)â•¯')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d55f2c3-c001-4821-b378-594528cc7d47",
   "metadata": {
    "id": "4d55f2c3-c001-4821-b378-594528cc7d47"
   },
   "source": [
    "# Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb93a23",
   "metadata": {
    "id": "0eb93a23"
   },
   "source": [
    "**The Training Loop**\n",
    "\n",
    "0. **Initialize** the weight and bias tensors with small random values.\n",
    "1. Draw a **mini-batch** of training samples `x` and corresponding labels `y`.\n",
    "2. **Forward pass**/**Inference**: the network makes a prediction `y_pred`.\n",
    "3. **Loss**: Calculate how much `y_pred` differs from `y`.\n",
    "4. **Backward pass**: using the gradient of the parameters with respect to the loss, update all parameters to lower the loss on this mini-batch.\n",
    "5. Exit or return to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d5d3da-faeb-4f1d-92db-a81b06a33266",
   "metadata": {
    "id": "45d5d3da-faeb-4f1d-92db-a81b06a33266"
   },
   "source": [
    "Learning is the process of minimising the **predictive loss** of a model.\n",
    "\n",
    "A neural network is trained by presenting **mini-batches** of samples and labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd81d63b",
   "metadata": {
    "id": "bd81d63b"
   },
   "source": [
    "Network parameters are adjusted in order to **reduce the difference** between prediction and target.\n",
    "\n",
    "The neural network implements a chain of **differentiable layer transformations**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97913581-1629-4019-8113-5e8116c6ce63",
   "metadata": {
    "id": "97913581-1629-4019-8113-5e8116c6ce63"
   },
   "source": [
    "The loss-parmeter gradient is efficiently computed using the **chain rule**: the **backpropagation** algorithm.\n",
    "\n",
    "\n",
    "The **optimizer** changes the parameters a small amount (our step) in the direction of the negative gradient."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Slideshow",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
