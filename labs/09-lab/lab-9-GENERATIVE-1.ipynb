{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Deep Learning 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "The lecture notebooks should be used as starter code: a good first step is to remove all that is not necessary for your enquiry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try building different language models. There are tens of thousands of texts in [Project Gutenberg](https://www.gutenberg.org). There is small but wide range of texts (and other data) [here](https://introcs.cs.princeton.edu/java/data/). Are you able to train models that generate realistic texts?\n",
    "\n",
    "[Karpathy](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) noted in his blog post that language models also work with code. What happens if you try and train a language model on the [TensorFlow codebase](https://github.com/tensorflow/tensorflow)?\n",
    "\n",
    "Other common sampling strategies on top of `temperature` are `top_k`, sampling only from the most probable `k` logits, and `top_p`, nucleus sampling, sampling only from the most probable logits which taken together amount for `p` probability (see [here](https://huggingface.co/blog/how-to-generate) for a more complete discussion). Can you implement these generation functionalities, for instance importing them from [here](https://github.com/akanyaani/gpt-2-tensorflow2.0/blob/master/sample.py#L11) and [here](https://github.com/akanyaani/gpt-2-tensorflow2.0/blob/master/sample.py#L27)?\n",
    "\n",
    "Note that if you add special markers into your dataset, e.g. a special character, like `‚ù°`, into the dataset, that allows you to control the behaviour of the trained net: if that character is used as an end point, splitting the dataset into parts, then in your generation loop that would allow you to stop generation, for instance, once that specific character has been produced. This logic is what fuels chatbots: if you have markers for the end of a reply/question by a user or by a bot, and you have a dataset of that, then you can train your model to output the 'end of reply' token after it has answered your question, which produces the _effect_ of conversation.\n",
    "\n",
    "Another approach is to think of what to do with a trained model: can you implement a typewriter effect when generating, perhaps adding a bit of randomness, using `time.sleep()`, to make it a bit irregular?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
