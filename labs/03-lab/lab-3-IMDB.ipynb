{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMe3XrY8j3HW"
   },
   "source": [
    "# The IMDB Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMe3XrY8j3HW"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMe3XrY8j3HW"
   },
   "source": [
    "### Colab Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMe3XrY8j3HW"
   },
   "source": [
    "Don't forget that you can link your notebook to your drive and save your work there. Then you can download and backup your models, reload them to keep training them, or upload datasets to your drive.\n",
    "\n",
    "```python\n",
    "import os\n",
    "import sys\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    os.chdir('drive/My Drive/') # 'My Drive' is the default name of Google Drives\n",
    "    os.listdir()\n",
    "\n",
    "# use os.chdir(\"my-directory\") # to change directory, and\n",
    "# os.listdir()                 # to list its contents\n",
    "# os.getcwd()                  # to get the name of the current directory\n",
    "# os.mkdir(\"my-new-dir\")       # to create a new directory\n",
    "# See: https://realpython.com/working-with-files-in-python/\n",
    "\n",
    "# You can also use bash commands directly, preceded by a bang\n",
    "# !ls\n",
    "# However, the following will *not* change the Python directory\n",
    "# the notebook points to (use os.chdir for that)!\n",
    "# !cd my-directory\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I17fLVAEj3Hf"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I17fLVAEj3Hf"
   },
   "source": [
    "## 1. Theory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "hdnmT1ukj3Hj"
   },
   "source": [
    "Make sure you understand the first video of 3Blue1Brown's introduction to neural nets, and ask questions if there's anything unclear.\n",
    "\n",
    "[3Blue1Brown | Backpropagation, intuitively | Deep Learning Chapter 3](https://www.youtube.com/watch?v=Ilg3gGewQ5U)\n",
    "\n",
    "The fourth video is optional.\n",
    "\n",
    "[3Blue1Brown | Backpropagation calculus | Deep Learning Chapter 4 ](https://www.youtube.com/watch?v=tIeHLnjs5U8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k8PaTLWAj3Hk",
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k8PaTLWAj3Hk",
    "tags": []
   },
   "source": [
    "## 2. Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "vdXG6cqgj3Hl"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8RmLzZjj3Hl"
   },
   "source": [
    "### For reproducible results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8RmLzZjj3Hl"
   },
   "source": [
    "In Keras ([source](https://keras.io/examples/keras_recipes/reproducibility_recipes/)):\n",
    "```python\n",
    "keras.utils.set_random_seed(812) # See below\n",
    "\n",
    "# If using TensorFlow, this will make GPU ops as deterministic as possible,\n",
    "# but it will affect the overall performance, so be mindful of that.\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "```\n",
    "\n",
    "Note: `keras.utils.set_random_seed` will do the following ([source](https://github.com/keras-team/keras/blob/d66ecf029e4864eeeff7e22408e82c95d63422d0/keras/src/utils/rng_utils.py#L58)):\n",
    "\n",
    "```python\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "# if tf as backend\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# if torch as backend\n",
    "torch.manual_seed(42)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7Qlwm4CZj3Hn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.0\n",
    "    return results\n",
    "\n",
    "\n",
    "# load the IMDB dataset (max review length\n",
    "(train_data, train_labels), (test_data, test_labels) = keras.datasets.imdb.load_data(num_words=10000)\n",
    "\n",
    "# preprocess\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)\n",
    "\n",
    "y_train = np.asarray(train_labels).astype(\"float32\")\n",
    "y_test = np.asarray(test_labels).astype(\"float32\")\n",
    "\n",
    "# split training set into train & validation\n",
    "partial_x_train = x_train[10000:]\n",
    "partial_y_train = y_train[10000:]\n",
    "x_val = x_train[:10000]\n",
    "y_val = y_train[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-LRmPQVsj3Ho"
   },
   "outputs": [],
   "source": [
    "# build\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.Input((10000,)))\n",
    "model.add(keras.layers.Dense(16, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(16, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6frpl3-oj3Hp"
   },
   "source": [
    "Sanity check, how does our network perform before training (use `.evaluate` on `partial_x_train, partial_y_train`). Is the accuracy a value you would expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PrXO-tbmj3Hq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5DL5xBF9j3Hr"
   },
   "source": [
    "Now we can train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fUXB8KNsj3Hr"
   },
   "outputs": [],
   "source": [
    "# save data from training into the 'history' object\n",
    "history = model.fit(\n",
    "    partial_x_train,\n",
    "    partial_y_train,\n",
    "    epochs=20,\n",
    "    batch_size=128,\n",
    "    validation_data=(x_val, y_val),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jh39kSJvj3Hs"
   },
   "source": [
    "### Visualise your results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jh39kSJvj3Hs"
   },
   "source": [
    "Thanks to these plots, it is easier to spot the epoch (epoch number) where our net reached peak performance (lowest *validation loss*/highest *validation accuracy*, prioritising accuracy if the two are not the same).\n",
    "\n",
    "Think about what would be a good strategy to keep your code as organised as possible as you run many experiments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x6glhVSrj3Hs"
   },
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SKLoJ4wnj3Ht"
   },
   "outputs": [],
   "source": [
    "history_dict[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WYdvEppwj3Ht"
   },
   "outputs": [],
   "source": [
    "loss = history_dict[\"loss\"]\n",
    "val_loss = history_dict[\"val_loss\"]\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "blue_dots = \"bo\"\n",
    "solid_blue_line = \"b\"\n",
    "\n",
    "plt.plot(epochs, loss, blue_dots, label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, solid_blue_line, label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tRfv86Pkj3Ht"
   },
   "outputs": [],
   "source": [
    "history_dict[\"accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8_kTIciSj3Hu"
   },
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc = history_dict[\"accuracy\"]\n",
    "val_acc = history_dict[\"val_accuracy\"]\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "blue_dots = \"bo\"\n",
    "solid_blue_line = \"b\"\n",
    "\n",
    "plt.plot(epochs, acc, blue_dots, label=\"Training acc\")\n",
    "plt.plot(epochs, val_acc, solid_blue_line, label=\"Validation acc\")\n",
    "plt.title(\"Training and validation acc\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you think of a way to making the plotting code above more re-usable, when training multiple models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oRX-pqK8j3Hm"
   },
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oRX-pqK8j3Hm"
   },
   "source": [
    "- Experiment with only one layer, then with more (deeper net);\n",
    "- Experiment with more or fewer hidden units – 32 units, 64 units etc.\n",
    "  - One nice challenge is to see how good your results get with a bigger network, then see if you can get to the same level with a smaller one by training longer, or tweaking the learning rate/changing the optimizer;\n",
    "  - Another is to see how good a result you can get with a fixed number of epochs (e.g. 5);\n",
    "- Experiment with replacing `relu` with `tanh` activations;\n",
    "- Try the [`Adam` optimizer](https://keras.io/api/optimizers/adam/): `optimizers.Adam(learning_rate=0.001)`\n",
    "- Investigate the effect of different learning rates;\n",
    "- Investigate the effect of a smaller (or bigger) batch size;\n",
    "- Train for more epochs, or, conversely, set a number of epochs (say 5), and see how good you can get your model in just these 5 epochs.\n",
    "\n",
    "**Think about how to record and organise your experiments in a neat way!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NfeRtAPsj3Hu",
    "tags": []
   },
   "source": [
    "## 3. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NfeRtAPsj3Hu",
    "tags": []
   },
   "source": [
    "Take your best network and train on **all the training data** (`x_train`, `y_train`), without a train/validation split, using the same hyperparameters (optimizer, learning rate, network size, etc.) as your best run, for the optimal number of epochs (looking at your best validation curves)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dw3mf_lZj3Hv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_IxgZLyj3Hv"
   },
   "source": [
    "Evaluate this last model on the test set (`x_test, y_test`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_9yNglQKj3Hv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAFOcnzmj3Hv"
   },
   "source": [
    "### Use your model (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAFOcnzmj3Hv"
   },
   "source": [
    "Can you import the lecture code used to test the model on a review, and see if you agree with its prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vDgNB8B-j3Hw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BR2RAkYRj3Hw"
   },
   "source": [
    "### Save and load models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BR2RAkYRj3Hw"
   },
   "source": [
    "To save and load models locally, you can use [the high-level API](https://www.tensorflow.org/tutorials/keras/save_and_load):\n",
    "```python\n",
    "model.save(\"my_imdb_model.keras\")\n",
    "```\n",
    "Later one, to reload it, use:\n",
    "```python\n",
    "reloaded_model = keras.models.load_model('my_imdb_model.keras')\n",
    "```\n",
    "\n",
    "It is also possible to save not just the model, but also the state of your optimiser, and every variable used during training, using the morer involved [checkpoints](https://www.tensorflow.org/guide/checkpoint#create_the_checkpoint_objects)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Additional experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One parameter you could study is the influence on the vocabulary size on the final results: you might then want to store the vocab size in a variable, and use that instead of the hard-coded `10000` that we have.\n",
    "- It would be entirely possible to approach the same problem as if you were detecting two classes (`0` for negative, `1` for positive), instead of using just a single output. In order to implement that, you would need:\n",
    "   1. two units in your final layer with a `softmax` activation;\n",
    "   2. `categorical_crossentropy` as a loss;\n",
    "   3. labels as one-hot vectors (using `keras.utils.to_categorical`);\n",
    "   4. and `np.argmax` (instead of `round`) to retrieve the likeliest class (the index) when making predictions.\n",
    "- Another line of enquiry is the study of the behaviour of your trained model:\n",
    "  - Are you able to modify existing reviews in a way that changes the initial prediction of your model? (One 'automated' way of doing that would be to remove a certain number of words from the review, and see how performance is impacted by that information loss.)\n",
    "  - Are you able to create a pipeline where you write your own review, or find one online, transform it into the appropriate format (remove punctuation, turn everything to lower case, convert to an array of integers using the dictionary yielded by `keras.datasets.imdb.get_word_index()` (beware of the shift by 3 induced by the reserved tokens for padding, start of sequence and unknown!), and see what prediction you get for it?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
